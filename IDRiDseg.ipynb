{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDRiDseg.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AcSma7_RfLnk",
        "EeZa_92kuQag",
        "_Na2DjTeexJd",
        "yrfnYhrjuYYQ",
        "BklkTaFzut95",
        "cHjVOXKMfXSa",
        "EqLtpXRru2kh",
        "p2JGpoMrtNx3",
        "ByIpNhUk-xQr",
        "CM2NY0sRHysI",
        "rnNDIjEh_iD3",
        "mjewf0kzHheN",
        "8hwL8cLfH89I"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO1hLRmdYRY52lJwMkBVVtb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimanshuAgarwal022/IDRiDSegmentation/blob/master/IDRiDseg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM9UH-6K18L8",
        "colab_type": "text"
      },
      "source": [
        "# Diabetic Retinopathy Lesion Segmentation.\n",
        "---\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) [2020] [Himanshu Agarwal]\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcSma7_RfLnk",
        "colab_type": "text"
      },
      "source": [
        "### load drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf1EOC0CtWWC",
        "colab_type": "code",
        "outputId": "9e20fb59-1936-4b7c-9a3c-3981b8edd276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeZa_92kuQag",
        "colab_type": "text"
      },
      "source": [
        "### convert ground truths to binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi6Fax6vuOoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "#from resizeimage import resizeimage\n",
        "import os, sys\n",
        "\n",
        "def cmp(a, b):\n",
        "    return (a > b) - (a < b) \n",
        "\n",
        "def resizeImage(infile,file, output_dir, size=(4288,2848)):\n",
        "  outfile = os.path.splitext(file)[0]\n",
        "  extension = os.path.splitext(file)[1]\n",
        "  #print(outfile)\n",
        "  #print(extension)\n",
        "  #print(infile)\n",
        "  #if (cmp(extension, \".jpg\")):\n",
        "    #print(\"dsd\")\n",
        "    #return\n",
        "\n",
        "  if infile != outfile:\n",
        "    try :\n",
        "      im = Image.open(infile)\n",
        "      gray = im.convert('L')\n",
        "      bw = gray.point(lambda x: 0 if x<50 else 255, '1')\n",
        "      # im = resizeimage.resize_cover(im, [960, 640])\n",
        "      bw.save(output_dir+outfile[:-3]+extension,\"TIFF\",quality=100)\n",
        "      #print(\"sucess\")\n",
        "    #except IOError:\n",
        "    #  print (\"cannot reduce image for \", infile)\n",
        "    except e:\n",
        "      print (e)\n",
        "    \n",
        "\n",
        "\n",
        "output_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "annot_dir = \"drive/My Drive/data/annotations/\"\n",
        "dir = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join(dir,output_dir)):\n",
        "  os.mkdir(output_dir)\n",
        "annot = os.path.join(dir,annot_dir)\n",
        "for file in os.listdir(annot):\n",
        "  #print(file)\n",
        "  resizeImage(os.path.join(annot,file),file,output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Na2DjTeexJd",
        "colab_type": "text"
      },
      "source": [
        "### CLAHE on input images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIIONbM_e4Jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "dir = os.getcwd()\n",
        "dir_data = os.path.join(dir,\"drive/My Drive/data/output/preimages/\")\n",
        "output_dir_data = os.path.join(dir,\"drive/My Drive/data/output/images/\")\n",
        "if not os.path.exists(os.path.join(dir,output_dir_data)):\n",
        "    os.mkdir(output_dir_data)\n",
        "\n",
        "gridsize = 8\n",
        "for file in os.listdir(dir_data):\n",
        "  bgr = cv2.imread(os.path.join(dir_data,file))\n",
        "  lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
        "  lab_planes = cv2.split(lab)\n",
        "  clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(gridsize,gridsize))\n",
        "  lab_planes[0] = clahe.apply(lab_planes[0])\n",
        "  lab = cv2.merge(lab_planes)\n",
        "  bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "  #if random.uniform(0, 1.0) > 0.75:\n",
        "  #  bgr = cv2.bitwise_not(bgr)\n",
        "  #  plt.imshow(bgr)\n",
        "  #  plt.show()\n",
        "  cv2.imwrite(os.path.join(output_dir_data,file),bgr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL-rKiA5hn_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#output_dir_data = os.path.join(dir,\"drive/My Drive/data/output/images/\")\n",
        "#for file in os.listdir(output_dir_data):\n",
        "#  bgr = cv2.imread(os.path.join(output_dir_data,file))\n",
        "#  rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
        "#  plt.imshow(rgb)\n",
        "#  plt.show()\n",
        "#  bgr = cv2.bitwise_not(bgr)\n",
        "#  plt.imshow(bgr)\n",
        "#  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrfnYhrjuYYQ",
        "colab_type": "text"
      },
      "source": [
        "### extract patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BhM3OxTuiWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "#from resizeimage import resizeimage\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "dir = os.getcwd()\n",
        "output_dir_data = \"drive/My Drive/data/output/patches/\"\n",
        "output_dir_mask = \"drive/My Drive/data/output/labels/\"\n",
        "if not os.path.exists(os.path.join(dir,output_dir_data)):\n",
        "    os.mkdir(output_dir_data)\n",
        "if not os.path.exists(os.path.join(dir,output_dir_mask)):\n",
        "    os.mkdir(output_dir_mask)\n",
        "\n",
        "dir_data = os.path.join(dir,\"drive/My Drive/data/output/images/\")\n",
        "dir_mask = os.path.join(dir,\"drive/My Drive/data/output/annotations/\")\n",
        "\n",
        "# im = Image.open(os.path.join(dir_mask,\"IDRiD_06.tif\"))\n",
        "# im_crop = im.crop((2000,0,2000+512,0+256))\n",
        "# im_crop.show()\n",
        "# image_np = np.array(im_crop)\n",
        "# print np.sum(image_np)\n",
        "\n",
        "negative_patches = []\n",
        "positive_count = 0\n",
        "\n",
        "for file in os.listdir(dir_mask):\n",
        "    outfile = os.path.splitext(file)[0]\n",
        "    extension = os.path.splitext(file)[1]\n",
        "    #if (cmp(extension, \".jpg\")):\n",
        "    #    continue\n",
        "    img = outfile + \".jpg\"\n",
        "    im = Image.open(os.path.join(dir_mask,file))\n",
        "    imd = Image.open(os.path.join(dir_data,img))\n",
        "    # image_np = np.array(im)\n",
        "    # print np.sum([True, True])\n",
        "    # im_crop = im.crop((1900,0,1900+512,0+512))\n",
        "    patch_id = 0\n",
        "    for i in range(10): #10 6\n",
        "    \tfor j in range(16): #16 9\n",
        "        top_y = i*256 #256 512\n",
        "        if (i==9): #9 5\n",
        "          top_y = 2336\n",
        "        top_x = j*256 #256 512\n",
        "        if (j==15): #15 8\n",
        "          top_x = 3776\n",
        "\n",
        "        im_crop = im.crop((top_x,top_y,top_x+512,top_y+512))\n",
        "        imd_crop = imd.crop((top_x,top_y,top_x+512,top_y+512))\n",
        "        im_crop.save(output_dir_mask+outfile+\"_p\"+str(patch_id)+extension,\"JPEG\",quality=100)\n",
        "        imd_crop.save(output_dir_data+outfile+\"_p\"+str(patch_id)+extension,\"JPEG\",quality=100)\n",
        "        if (np.sum(np.array(im_crop)) < 100):\n",
        "          negative_patches.append(output_dir_mask+outfile+\"_p\"+str(patch_id)+extension)\n",
        "        else:\n",
        "          positive_count += 1\n",
        "\n",
        "        patch_id += 1\n",
        "\n",
        "negative_patches = np.array(negative_patches)\n",
        "# np.savetxt(\"negative.csv\", negative_patches, delimiter=\",\", fmt=\"%s\")\n",
        "\n",
        "negative_count = negative_patches.size\n",
        "delete_count = negative_count - 4*positive_count\n",
        "np.random.shuffle(negative_patches)\n",
        "split_idx = delete_count\n",
        "delete_patches = negative_patches[:split_idx]\n",
        "\n",
        "for idx in range(delete_patches.size):\n",
        "    os.remove(delete_patches[idx])\n",
        "    os.remove(os.path.join(output_dir_data,delete_patches[idx][34:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BklkTaFzut95",
        "colab_type": "text"
      },
      "source": [
        "### split in train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krh5Z4OHuvBu",
        "colab_type": "code",
        "outputId": "72c55ae1-8c28-446d-eade-1335338daa7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "dir = \"drive/My Drive/data/output/patches/\"\n",
        "#dir = \"drive/My Drive/data/output/images/\"\n",
        "image_paths = os.listdir(dir)\n",
        "\n",
        "length = len(image_paths)\n",
        "img_paths = np.empty(length, dtype=object)\n",
        "\n",
        "i=0\n",
        "for file in image_paths:\n",
        "\timg_paths[i] = \"drive/My Drive/data/output/patches/\" + file\n",
        "\t#img_paths[i] = \"drive/My Drive/data/output/images/\" + file\n",
        "\t#print (img_paths[i])\n",
        "\ti+=1\n",
        "\n",
        "# print (img_paths)\n",
        "np.random.shuffle(img_paths)\n",
        "#split_idx = int(img_paths.shape[0] * 1)\n",
        "split_idx = 11534\n",
        "train_paths = img_paths[:split_idx]\n",
        "test_paths = img_paths[split_idx:]\n",
        "\n",
        "train_paths_ = np.copy(train_paths)\n",
        "test_paths_ = np.copy(test_paths)\n",
        "print(\"train set: \",train_paths.size)\n",
        "print(\"test set: \",test_paths.size)\n",
        "for i in range(train_paths.size):\n",
        "\ttrain_paths_[i] = train_paths[i][35:]\n",
        "\t#train_paths_[i] = train_paths[i][34:]\n",
        "\ttrain_paths_[i] = \"drive/My Drive/data/output/labels/\" + train_paths_[i]\n",
        "\t#train_paths_[i] = \"drive/My Drive/data/output/annotations/\" + train_paths_[i]\n",
        "\t#print (train_paths_[i])\n",
        "\n",
        "#print (\"split\")\n",
        "\n",
        "for i in range(test_paths.size):\n",
        "\ttest_paths_[i] = test_paths[i][35:]\n",
        "\t#test_paths_[i] = test_paths[i][34:]\n",
        "\ttest_paths_[i] = \"drive/My Drive/data/output/labels/\" + test_paths_[i]\n",
        "\t#test_paths_[i] = \"drive/My Drive/data/output/annotations/\" + test_paths_[i]\n",
        "\t#print (test_paths_[i])\n",
        "\n",
        "train_csv = np.stack((train_paths,train_paths_), axis=1)\n",
        "test_csv = np.stack((test_paths,test_paths_), axis=1)\n",
        "\n",
        "np.savetxt(\"train.csv\", train_csv, delimiter=\",\", fmt=\"%s\")\n",
        "np.savetxt(\"test.csv\", test_csv, delimiter=\",\", fmt=\"%s\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train set:  11534\n",
            "test set:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjVOXKMfXSa",
        "colab_type": "text"
      },
      "source": [
        "### load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YpGHNnLuAt9",
        "colab_type": "code",
        "outputId": "dcd8132f-f5ae-45f0-fcdd-fe89f1910423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#import os\n",
        "#os.getcwd()\n",
        "#os.listdir()\n",
        "#os.path.exists('drive/My Drive/')\n",
        "#for roots,dirs,files in os.walk('drive/My Drive'):               \n",
        "#                    print(roots,dirs,files)\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "TensorFlow Version: 1.15.2\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdVEq0W1we7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJis0dgqxwGB",
        "colab_type": "code",
        "outputId": "2005613f-bf1e-44c7-c8b0-0c61609123f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=ee2b3d234e82bb1c13f028526f9f680f5971df1f7db91a5bc6c48c72b186221a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.4 GB  | Proc size: 557.2 MB\n",
            "GPU RAM Free: 16015MB | Used: 265MB | Util   2% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLtpXRru2kh",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIVvZ3JxsepC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm 'drive/My Drive/data/models' -rf\n",
        "#!rm 'drive/My Drive/data/logs' -rf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jHwMuhTxgBi",
        "colab_type": "code",
        "outputId": "43824013-f102-4ad3-9f4f-c9d73270042e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def image_augmentation(image, mask):\n",
        "    \"\"\"Returns (maybe) augmented images\n",
        "    (1) Random flip (left <--> right)\n",
        "    (2) Random flip (up <--> down)\n",
        "    (3) Random brightness\n",
        "    (4) Random hue\n",
        "    Args:\n",
        "        image (3-D Tensor): Image tensor of (H, W, C)\n",
        "        mask (3-D Tensor): Mask image tensor of (H, W, 1)\n",
        "    Returns:\n",
        "        image: Maybe augmented image (same shape as input `image`)\n",
        "        mask: Maybe augmented mask (same shape as input `mask`)\n",
        "    \"\"\"\n",
        "    concat_image = tf.concat([image, mask], axis=-1)\n",
        "\n",
        "    maybe_flipped = tf.image.random_flip_left_right(concat_image)\n",
        "    maybe_flipped = tf.image.random_flip_up_down(concat_image)\n",
        "\n",
        "    image = maybe_flipped[:, :, :-1]\n",
        "    mask = maybe_flipped[:, :, -1:]\n",
        "\n",
        "    image = tf.image.random_brightness(image, 0.7)\n",
        "    image = tf.image.random_hue(image, 0.3)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def get_image_mask(queue, augmentation=True):\n",
        "    \"\"\"Returns `image` and `mask`\n",
        "    Input pipeline:\n",
        "        Queue -> CSV -> FileRead -> Decode JPEG\n",
        "    (1) Queue contains a CSV filename\n",
        "    (2) Text Reader opens the CSV\n",
        "        CSV file contains two columns\n",
        "        [\"path/to/image.jpg\", \"path/to/mask.jpg\"]\n",
        "    (3) File Reader opens both files\n",
        "    (4) Decode JPEG to tensors\n",
        "    Notes:\n",
        "        height, width = 640, 960\n",
        "    Returns\n",
        "        image (3-D Tensor): (640, 960, 3)\n",
        "        mask (3-D Tensor): (640, 960, 1)\n",
        "    \"\"\"\n",
        "    text_reader = tf.TextLineReader(skip_header_lines=1)\n",
        "    _, csv_content = text_reader.read(queue)\n",
        "\n",
        "    image_path, mask_path = tf.decode_csv(\n",
        "        csv_content, record_defaults=[[\"\"], [\"\"]])\n",
        "\n",
        "    image_file = tf.read_file(image_path)\n",
        "    mask_file = tf.read_file(mask_path)\n",
        "\n",
        "    image = tf.image.decode_jpeg(image_file, channels=3)\n",
        "    image.set_shape([512, 512, 3])\n",
        "    image = tf.cast(image, tf.float32)\n",
        "\n",
        "    mask = tf.image.decode_jpeg(mask_file, channels=1)\n",
        "    mask.set_shape([512, 512, 1])\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "    mask = mask / (tf.reduce_max(mask) + 1e-7)\n",
        "\n",
        "    if augmentation:\n",
        "        image, mask = image_augmentation(image, mask)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def conv_conv_pool(input_,\n",
        "                   n_filters,\n",
        "                   training,\n",
        "                   flags,\n",
        "                   name,\n",
        "                   pool=True,\n",
        "                   activation=tf.nn.relu):\n",
        "    \"\"\"{Conv -> BN -> RELU}x2 -> {Pool, optional}\n",
        "    Args:\n",
        "        input_ (4-D Tensor): (batch_size, H, W, C)\n",
        "        n_filters (list): number of filters [int, int]\n",
        "        training (1-D Tensor): Boolean Tensor\n",
        "        name (str): name postfix\n",
        "        pool (bool): If True, MaxPool2D\n",
        "        activation: Activaion functions\n",
        "    Returns:\n",
        "        net: output of the Convolution operations\n",
        "        pool (optional): output of the max pooling operations\n",
        "    \"\"\"\n",
        "    net = input_\n",
        "\n",
        "    with tf.variable_scope(\"layer{}\".format(name)):\n",
        "        for i, F in enumerate(n_filters):\n",
        "            net = tf.layers.conv2d(\n",
        "                net,\n",
        "                F, (3, 3),\n",
        "                activation=None,\n",
        "                padding='same',\n",
        "                kernel_regularizer=tf.contrib.layers.l2_regularizer(flags.reg),\n",
        "                name=\"conv_{}\".format(i + 1))\n",
        "            net = tf.layers.batch_normalization(\n",
        "                net, training=training, name=\"bn_{}\".format(i + 1))\n",
        "            net = activation(net, name=\"relu{}_{}\".format(name, i + 1))\n",
        "\n",
        "        if pool is False:\n",
        "            return net\n",
        "\n",
        "        pool = tf.layers.max_pooling2d(\n",
        "            net, (2, 2), strides=(2, 2), name=\"pool_{}\".format(name))\n",
        "\n",
        "        return net, pool\n",
        "\n",
        "\n",
        "def upconv_concat(inputA, input_B, n_filter, flags, name):\n",
        "    \"\"\"Upsample `inputA` and concat with `input_B`\n",
        "    Args:\n",
        "        input_A (4-D Tensor): (N, H, W, C)\n",
        "        input_B (4-D Tensor): (N, 2*H, 2*H, C2)\n",
        "        name (str): name of the concat operation\n",
        "    Returns:\n",
        "        output (4-D Tensor): (N, 2*H, 2*W, C + C2)\n",
        "    \"\"\"\n",
        "    up_conv = upconv_2D(inputA, n_filter, flags, name)\n",
        "\n",
        "    return tf.concat(\n",
        "        [up_conv, input_B], axis=-1, name=\"concat_{}\".format(name))\n",
        "\n",
        "\n",
        "def upconv_2D(tensor, n_filter, flags, name):\n",
        "    \"\"\"Up Convolution `tensor` by 2 times\n",
        "    Args:\n",
        "        tensor (4-D Tensor): (N, H, W, C)\n",
        "        n_filter (int): Filter Size\n",
        "        name (str): name of upsampling operations\n",
        "    Returns:\n",
        "        output (4-D Tensor): (N, 2 * H, 2 * W, C)\n",
        "    \"\"\"\n",
        "\n",
        "    return tf.layers.conv2d_transpose(\n",
        "        tensor,\n",
        "        filters=n_filter,\n",
        "        kernel_size=2,\n",
        "        strides=2,\n",
        "        kernel_regularizer=tf.contrib.layers.l2_regularizer(flags.reg),\n",
        "        name=\"upsample_{}\".format(name))\n",
        "\n",
        "\n",
        "def make_unet(X, training, flags=None):\n",
        "    \"\"\"Build a U-Net architecture\n",
        "    Args:\n",
        "        X (4-D Tensor): (N, H, W, C)\n",
        "        training (1-D Tensor): Boolean Tensor is required for batchnormalization layers\n",
        "    Returns:\n",
        "        output (4-D Tensor): (N, H, W, C)\n",
        "            Same shape as the `input` tensor\n",
        "    Notes:\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "    \"\"\"\n",
        "    net = X / 127.5 - 1\n",
        "    conv1, pool1 = conv_conv_pool(net, [16, 16], training, flags, name=1)\n",
        "    conv2, pool2 = conv_conv_pool(pool1, [32, 32], training, flags, name=2)\n",
        "    conv3, pool3 = conv_conv_pool(pool2, [64, 64], training, flags, name=3)\n",
        "    conv4, pool4 = conv_conv_pool(pool3, [128, 128], training, flags, name=4)\n",
        "    conv5, pool5 = conv_conv_pool(pool4, [256, 256], training, flags, name=5)\n",
        "    conv6, pool6 = conv_conv_pool(pool5, [512, 512], training, flags, name=6)\n",
        "    conv7 = conv_conv_pool(\n",
        "        pool6, [1024, 1024], training, flags, name=7, pool=False)\n",
        "\n",
        "    up8 = upconv_concat(conv7, conv6, 512, flags, name=8)\n",
        "    conv8 = conv_conv_pool(up8, [512, 512], training, flags, name=8, pool=False)\n",
        "\n",
        "    up9 = upconv_concat(conv8, conv5, 256, flags, name=9)\n",
        "    conv9 = conv_conv_pool(up9, [256, 256], training, flags, name=9, pool=False)\n",
        "\n",
        "    up10 = upconv_concat(conv9, conv4, 128, flags, name=10)\n",
        "    conv10 = conv_conv_pool(up10, [128, 128], training, flags, name=10, pool=False)\n",
        "\n",
        "    up11 = upconv_concat(conv10, conv3, 64, flags, name=11)\n",
        "    conv11 = conv_conv_pool(up11, [64, 64], training, flags, name=11, pool=False)\n",
        "\n",
        "    up12 = upconv_concat(conv11, conv2, 32, flags, name=12)\n",
        "    conv12 = conv_conv_pool(up12, [32, 32], training, flags, name=12, pool=False)\n",
        "\n",
        "    up13 = upconv_concat(conv12, conv1, 16, flags, name=13)\n",
        "    conv13 = conv_conv_pool(up13, [16, 16], training, flags, name=13, pool=False)\n",
        "\n",
        "    # return tf.layers.conv2d(\n",
        "    #     conv13,\n",
        "    #     1, (1, 1),\n",
        "    #     name='final',\n",
        "    #     activation=tf.nn.sigmoid,\n",
        "    #     padding='same')\n",
        "    return tf.layers.conv2d(conv13,1, (1, 1),name='final',activation=None,padding='same')\n",
        "\n",
        "def BCE_(y_pred, y_true):\n",
        "    # weight ratio = 9:1\n",
        "    # 9-1=8\n",
        "    class_weights = tf.constant([8],dtype=tf.float32)\n",
        "    tensor_one = tf.constant([1],dtype=tf.float32)\n",
        "\n",
        "    pred_flat = tf.reshape(y_pred, [-1, 1])\n",
        "    true_flat = tf.reshape(y_true, [-1, 1])\n",
        "\n",
        "    weight_map = tf.multiply(true_flat, class_weights)\n",
        "    weight_map = tf.add(weight_map, tensor_one)\n",
        "\n",
        "    loss_map = tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_flat, labels=true_flat)\n",
        "    loss_map = tf.multiply(loss_map, weight_map)\n",
        "    loss = tf.reduce_mean(loss_map)\n",
        "    return loss\n",
        "\n",
        "def IOU_(y_pred, y_true):\n",
        "    \"\"\"Returns a (approx) IOU score\n",
        "    intesection = y_pred.flatten() * y_true.flatten()\n",
        "    Then, IOU = 2 * intersection / (y_pred.sum() + y_true.sum() + 1e-7) + 1e-7\n",
        "    Args:\n",
        "        y_pred (4-D array): (N, H, W, 1)\n",
        "        y_true (4-D array): (N, H, W, 1)\n",
        "    Returns:\n",
        "        float: IOU score\n",
        "    \"\"\"\n",
        "    H, W, _ = y_pred.get_shape().as_list()[1:]\n",
        "    threshold = 0.7\n",
        "    pred_flat = tf.reshape(y_pred, [-1, H * W])\n",
        "    true_flat = tf.reshape(y_true, [-1, H * W])\n",
        "    pred = tf.cast(pred_flat > threshold, dtype=tf.float32)\n",
        "    true = tf.cast(true_flat > threshold, dtype=tf.float32)\n",
        "    intersection = tf.reduce_sum(pred * true, axis=1) + 1e-7\n",
        "    denominator = tf.reduce_sum(pred, axis=1) + tf.reduce_sum(true, axis=1) + 1e-7\n",
        "\n",
        "    return tf.reduce_mean(intersection / denominator)\n",
        "\n",
        "\n",
        "def make_train_op(y_pred, y_true):\n",
        "    \"\"\"Returns a training operation\n",
        "    Args:\n",
        "        y_pred (4-D Tensor): (N, H, W, 1)\n",
        "        y_true (4-D Tensor): (N, H, W, 1)\n",
        "    Returns:\n",
        "        train_op: minimize operation\n",
        "    \"\"\"\n",
        "    # loss = -IOU_(y_pred, y_true)\n",
        "    loss = BCE_(y_pred, y_true)\n",
        "\n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "    # optim = tf.train.AdamOptimizer()\n",
        "    optim = tf.train.AdamOptimizer(1e-4)\n",
        "    return optim.minimize(loss, global_step=global_step)\n",
        "class flags:\n",
        "  epochs = 100\n",
        "  batch_size = 8\n",
        "  logdir = \"drive/My Drive/data/logs/\"\n",
        "  reg = 0.1\n",
        "  ckdir = \"drive/My Drive/data/models/\"\n",
        "\n",
        "\n",
        "'''def read_flags():\n",
        "    \"\"\"Returns flags\"\"\"\n",
        "\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", default=1, type=int, help=\"Number of epochs\")\n",
        "\n",
        "    parser.add_argument(\"--batch-size\", default=8, type=int, help=\"Batch size\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--logdir\", default=\"logdir\", help=\"Tensorboard log directory\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--reg\", type=float, default=0.1, help=\"L2 Regularizer Term\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--ckdir\", default=\"models\", help=\"Checkpoint directory\")\n",
        "\n",
        "    flags = parser.parse_args()\n",
        "    return flags'''\n",
        "\n",
        "\n",
        "def main():\n",
        "    train = pd.read_csv(\"./train.csv\")\n",
        "    n_train = train.shape[0]\n",
        "\n",
        "    test = pd.read_csv(\"./test.csv\")\n",
        "    n_test = test.shape[0]\n",
        "\n",
        "    current_time = time.strftime(\"%m/%d/%H/%M/%S\")\n",
        "    train_logdir = os.path.join(flags.logdir, \"train\", current_time)\n",
        "    test_logdir = os.path.join(flags.logdir, \"test\", current_time)\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    X = tf.placeholder(tf.float32, shape=[None, 512, 512, 3], name=\"X\")\n",
        "    y = tf.placeholder(tf.float32, shape=[None, 512, 512, 1], name=\"y\")\n",
        "    mode = tf.placeholder(tf.bool, name=\"mode\")\n",
        "\n",
        "    pred = make_unet(X, mode, flags)\n",
        "\n",
        "    tf.add_to_collection(\"inputs\", X)\n",
        "    tf.add_to_collection(\"inputs\", mode)\n",
        "    tf.add_to_collection(\"outputs\", pred)\n",
        "\n",
        "    tf.summary.histogram(\"Predicted Mask\", pred)\n",
        "    tf.summary.image(\"Predicted Mask\", pred)\n",
        "\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        train_op = make_train_op(pred, y)\n",
        "\n",
        "    IOU_op = IOU_(pred, y)\n",
        "    IOU_op = tf.Print(IOU_op, [IOU_op])\n",
        "    tf.summary.scalar(\"IOU\", IOU_op)\n",
        "\n",
        "    train_csv = tf.train.string_input_producer(['train.csv'])\n",
        "    test_csv = tf.train.string_input_producer(['test.csv'])\n",
        "    train_image, train_mask = get_image_mask(train_csv)\n",
        "    test_image, test_mask = get_image_mask(test_csv, augmentation=False)\n",
        "\n",
        "    X_batch_op, y_batch_op = tf.train.shuffle_batch(\n",
        "        [train_image, train_mask],\n",
        "        batch_size=flags.batch_size,\n",
        "        capacity=flags.batch_size * 5,\n",
        "        min_after_dequeue=flags.batch_size * 2,\n",
        "        allow_smaller_final_batch=True)\n",
        "\n",
        "    X_test_op, y_test_op = tf.train.batch(\n",
        "        [test_image, test_mask],\n",
        "        batch_size=flags.batch_size,\n",
        "        capacity=flags.batch_size * 2,\n",
        "        allow_smaller_final_batch=True)\n",
        "\n",
        "    summary_op = tf.summary.merge_all()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        train_summary_writer = tf.summary.FileWriter(train_logdir, sess.graph)\n",
        "        test_summary_writer = tf.summary.FileWriter(test_logdir)\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        if os.path.exists(flags.ckdir):\n",
        "            latest_check_point = tf.train.latest_checkpoint(flags.ckdir)\n",
        "            saver.restore(sess, latest_check_point)\n",
        "            print('model restored!')\n",
        "\n",
        "        else:\n",
        "            #try:\n",
        "            #    os.rmdir(flags.ckdir)\n",
        "            #except IOError:\n",
        "            #    pass\n",
        "            os.mkdir(flags.ckdir)\n",
        "\n",
        "        try:\n",
        "            global_step = tf.train.get_global_step(sess.graph)\n",
        "\n",
        "            coord = tf.train.Coordinator()\n",
        "            threads = tf.train.start_queue_runners(coord=coord)\n",
        "            start = time.time()\n",
        "            for epoch in range(90,flags.epochs):\n",
        "                print('%d epochs in %fs' % (epoch, 62539+time.time()-start))\n",
        "                for step in range(0, n_train, flags.batch_size):\n",
        "                    #print(\"%d train steps in %fs\" %(step, time.time()-start))\n",
        "                    X_batch, y_batch = sess.run([X_batch_op, y_batch_op])\n",
        "\n",
        "                    _, step_iou, step_summary, global_step_value = sess.run(\n",
        "                        [train_op, IOU_op, summary_op, global_step],\n",
        "                        feed_dict={X: X_batch,\n",
        "                                   y: y_batch,\n",
        "                                   mode: True})\n",
        "\n",
        "                    train_summary_writer.add_summary(step_summary,\n",
        "                                                     global_step_value)\n",
        "\n",
        "                total_iou = 0\n",
        "                for step in range(0, n_test, flags.batch_size):\n",
        "                    #print(\"%d test steps in %fs\" %(step, time.time()-start))\n",
        "                    X_test, y_test = sess.run([X_test_op, y_test_op])\n",
        "                    step_iou, step_summary = sess.run(\n",
        "                        [IOU_op, summary_op],\n",
        "                        feed_dict={X: X_test,\n",
        "                                   y: y_test,\n",
        "                                   mode: False})\n",
        "\n",
        "                    total_iou += step_iou * X_test.shape[0]\n",
        "\n",
        "                    test_summary_writer.add_summary(step_summary,(epoch + 1) * (step + 1))\n",
        "\n",
        "                #saver.save(sess, \"{}/model.ckpt\".format(flags.ckdir))\n",
        "                #print('the %d epoch in %fs, saved successfully' % (epoch, time.time()-start))\n",
        "\n",
        "        finally:\n",
        "            coord.request_stop()\n",
        "            coord.join(threads)\n",
        "            saver.save(sess, \"{}/model.ckpt\".format(flags.ckdir))\n",
        "            print('model saved successfully')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #flags = read_flags()\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name Predicted Mask is illegal; using Predicted_Mask instead.\n",
            "INFO:tensorflow:Summary name Predicted Mask is illegal; using Predicted_Mask instead.\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/data/models/model.ckpt\n",
            "model restored!\n",
            "90 epochs in 62539.000005s\n",
            "91 epochs in 63012.843219s\n",
            "92 epochs in 63480.010369s\n",
            "93 epochs in 63947.371985s\n",
            "94 epochs in 64414.004312s\n",
            "95 epochs in 64883.230457s\n",
            "96 epochs in 65350.314995s\n",
            "97 epochs in 65820.104939s\n",
            "98 epochs in 66288.175114s\n",
            "99 epochs in 66757.795235s\n",
            "model saved successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQOF55DOjN_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir 'drive/My Drive/data/logs'\n",
        "#%reload_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2JGpoMrtNx3",
        "colab_type": "text"
      },
      "source": [
        "### Test on a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXJmE8wYtUJ2",
        "colab_type": "code",
        "outputId": "b50675a1-3018-42ed-a51c-9d2e17fd8f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage.measurements import label\n",
        "from PIL import Image\n",
        "from scipy.special import expit\n",
        "\n",
        "saver = tf.train.import_meta_graph(\"drive/My Drive/data/models/model.ckpt.meta\")\n",
        "sess = tf.InteractiveSession()\n",
        "saver.restore(sess, \"drive/My Drive/data/models/model.ckpt\")\n",
        "X, mode = tf.get_collection(\"inputs\")[:2]\n",
        "pred = tf.get_collection(\"outputs\")[0]\n",
        "\n",
        "def read_image(image_path, gray=False):\n",
        "    \"\"\"Returns an image array\n",
        "    Args:\n",
        "        image_path (str): Path to image.jpg\n",
        "    Returns:\n",
        "        3-D array: RGB numpy image array\n",
        "    \"\"\"\n",
        "    if gray:\n",
        "        return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    image = cv2.imread(image_path)    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def pipeline(image, threshold=0.5, image_WH=(512, 512)):\n",
        "    image = np.copy(image)\n",
        "    H, W, C = image.shape\n",
        "    \n",
        "    if (W, H) != image_WH:\n",
        "        image = cv2.resize(image, image_WH)\n",
        "    \n",
        "    mask_pred = sess.run(pred, feed_dict={X: np.expand_dims(image, 0),\n",
        "                                          mode: False})\n",
        "    \n",
        "    mask_pred = np.squeeze(mask_pred)\n",
        "    mask_pred = expit(mask_pred)\n",
        "    # mask_pred = mask_pred > threshold\n",
        "    return mask_pred\n",
        "\n",
        "image_path = \"drive/My Drive/data/output/images/IDRiD_17.jpg\"\n",
        "image = read_image(image_path)\n",
        "predicted_image = np.zeros((2848, 4288), dtype=float)\n",
        "\n",
        "for i in range(10): #10 6\n",
        "  for j in range(16): #16 9\n",
        "    top_y = i*256 #256 512\n",
        "    if (i==9): #9 5\n",
        "      top_y = 2336\n",
        "    top_x = j*256 #256 512\n",
        "    if (j==15): #15 8\n",
        "      top_x = 3776\n",
        "\n",
        "    image_crop = image[top_y:top_y+512, top_x:top_x+512]\n",
        "    predicted_crop = pipeline(image_crop)\n",
        "    predicted_image[top_y:top_y+512, top_x:top_x+512] = np.maximum(predicted_image[top_y:top_y+512, top_x:top_x+512], predicted_crop)\n",
        "\n",
        "threshold = 0.7\n",
        "predicted_image = predicted_image > threshold\n",
        "(unique, counts) = np.unique(predicted_image.astype('uint8')*255, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print(frequencies)\n",
        "\n",
        "predicted_save = Image.fromarray((predicted_image.astype('uint8'))*255)\n",
        "predicted_save.save(\"test_predicted.jpg\", \"JPEG\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/queue_runner_impl.py:391: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/data/models/model.ckpt\n",
            "[[       0 10224722]\n",
            " [     255  1987502]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByIpNhUk-xQr",
        "colab_type": "text"
      },
      "source": [
        "### Generate probability maps for the dataset using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zed1ERU_FyF",
        "colab_type": "code",
        "outputId": "67f45da6-6b60-459b-c072-9d364ff20b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage.measurements import label\n",
        "from PIL import Image\n",
        "from scipy.special import expit\n",
        "import os, sys\n",
        "\n",
        "saver = tf.train.import_meta_graph(\"drive/My Drive/data/models/model.ckpt.meta\")\n",
        "sess = tf.InteractiveSession()\n",
        "saver.restore(sess, \"drive/My Drive/data/models/model.ckpt\")\n",
        "X, mode = tf.get_collection(\"inputs\")[:2]\n",
        "pred = tf.get_collection(\"outputs\")[0]\n",
        "\n",
        "def read_image(image_path, gray=False):\n",
        "    \"\"\"Returns an image array\n",
        "    Args:\n",
        "        image_path (str): Path to image.jpg\n",
        "    Returns:\n",
        "        3-D array: RGB numpy image array\n",
        "    \"\"\"\n",
        "    if gray:\n",
        "        return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    image = cv2.imread(image_path)    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def pipeline(image, image_WH=(512, 512)):\n",
        "    image = np.copy(image)\n",
        "    H, W, C = image.shape\n",
        "    \n",
        "    if (W, H) != image_WH:\n",
        "        image = cv2.resize(image, image_WH)\n",
        "    \n",
        "    mask_pred = sess.run(pred, feed_dict={X: np.expand_dims(image, 0),\n",
        "                                          mode: False})\n",
        "    \n",
        "    mask_pred = np.squeeze(mask_pred)\n",
        "    mask_pred = expit(mask_pred)\n",
        "    # mask_pred = mask_pred > threshold\n",
        "    return mask_pred\n",
        "\n",
        "output_dir = \"drive/My Drive/data/output/prob/\"\n",
        "dir = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join(dir,output_dir)):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "for image_path in os.listdir(os.path.join(dir,\"drive/My Drive/data/output/images/\")):\n",
        "    image = read_image(\"drive/My Drive/data/output/images/\"+image_path)\n",
        "    predicted_image = np.zeros((2848, 4288), dtype=float)\n",
        "\n",
        "    for i in range(10): #10 6\n",
        "      for j in range(16): #16 9\n",
        "        top_y = i*256 #256 512\n",
        "        if (i==9): #9 5\n",
        "          top_y = 2336\n",
        "        top_x = j*256 #256 512\n",
        "        if (j==15): #15 8\n",
        "          top_x = 3776\n",
        "\n",
        "        image_crop = image[top_y:top_y+512, top_x:top_x+512]\n",
        "        predicted_crop = pipeline(image_crop)\n",
        "        predicted_image[top_y:top_y+512, top_x:top_x+512] = np.maximum(predicted_image[top_y:top_y+512, top_x:top_x+512], predicted_crop)\n",
        "\n",
        "    # threshold = 0.5\n",
        "    # predicted_image = predicted_image > threshold\n",
        "    #(unique, counts) = np.unique((predicted_image*255).astype('uint8'), return_counts=True)\n",
        "    #frequencies = np.asarray((unique, counts)).T\n",
        "    #print(frequencies)\n",
        "    predicted_save = Image.fromarray((predicted_image*255).astype('uint8'))\n",
        "    predicted_save.save(output_dir+image_path, \"JPEG\", quality=100)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/My Drive/data/models/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM2NY0sRHysI",
        "colab_type": "text"
      },
      "source": [
        "### plot FROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq8hygaTHxv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "fc83bffd-4f6e-4a40-9828-ebd1dbb384b0"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_size=81\n",
        "gt_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "prob_dir = \"drive/My Drive/data/output/prob/\"\n",
        "true_p=0\n",
        "actual_p=0\n",
        "pred_p=0\n",
        "false_p=0\n",
        "\n",
        "thresh_list = [0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999, 0.9999, 0.99999, 1]\n",
        "\n",
        "dir = os.getcwd()\n",
        "thresh_size = len(thresh_list)\n",
        "sn = np.empty(thresh_size, dtype=float)\n",
        "fppi = np.empty(thresh_size, dtype=float)\n",
        "thresh_array = np.array(thresh_list)\n",
        "\n",
        "for th in range(thresh_size):\n",
        "\tthreshold = thresh_array[th]\n",
        "\tprint (threshold)\n",
        "\ttrue_p=0\n",
        "\tactual_p=0\n",
        "\tpred_p=0\n",
        "\tfalse_p=0\n",
        "\n",
        "\tfor image_path in os.listdir(os.path.join(dir,gt_dir)):\n",
        "\t\t# print image_path\n",
        "\t\tim_gt = Image.open(gt_dir+image_path)\n",
        "\t\timg = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "\t\tim_prob = Image.open(prob_dir+img)\n",
        "\t\tarr_gt = np.array(im_gt)\n",
        "\t\t#(unique, counts) = np.unique(arr_gt, return_counts=True)\n",
        "\t\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t\t#print(\"arr_gt: \",frequencies)\n",
        "\t\tarr_prob = (np.array(im_prob)).astype(float)/255\n",
        "\t\t#(unique, counts) = np.unique(arr_prob, return_counts=True)\n",
        "\t\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t\t#print(\"arr_prob: \",frequencies)\n",
        "\t\tarr_pred = (arr_prob > threshold).astype('uint8')\n",
        "\t\t#(unique, counts) = np.unique(arr_pred, return_counts=True)\n",
        "\t\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t\t#print(\"arr_pred: \",frequencies)\n",
        "\t\ttp = np.sum(np.logical_and(arr_gt, arr_pred))\n",
        "\t\tap = np.sum(arr_gt)\n",
        "\t\tpp = np.sum(arr_pred)\n",
        "\t\ttrue_p += tp\n",
        "\t\tactual_p += ap\n",
        "\t\tpred_p += pp\n",
        "\t\tfalse_p += (pp-tp)\n",
        "\n",
        "\tsn[th] = float(true_p)/float(actual_p)\n",
        "\tprint (\"sn: \", sn[th])\n",
        "\tfppi[th] = float(false_p)/float(training_size)\n",
        "\tprint (\"fppi: \", fppi[th])\n",
        "\n",
        "plt.plot(fppi, sn)\n",
        "plt.ylabel('SN')\n",
        "plt.xlabel('FPs per image')\n",
        "plt.savefig('drive/My Drive/data/froc.png')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "sn:  0.9997918539315748\n",
            "fppi:  433847.8024691358\n",
            "1e-05\n",
            "sn:  0.9997918539315748\n",
            "fppi:  433847.8024691358\n",
            "0.0001\n",
            "sn:  0.9997918539315748\n",
            "fppi:  433847.8024691358\n",
            "0.001\n",
            "sn:  0.9997918539315748\n",
            "fppi:  433847.8024691358\n",
            "0.01\n",
            "sn:  0.9994911652226554\n",
            "fppi:  298734.4567901235\n",
            "0.1\n",
            "sn:  0.9943516045316544\n",
            "fppi:  163824.34567901236\n",
            "0.2\n",
            "sn:  0.9918498585006056\n",
            "fppi:  133057.64197530865\n",
            "0.3\n",
            "sn:  0.9889898217071691\n",
            "fppi:  113391.08641975309\n",
            "0.4\n",
            "sn:  0.985346916103857\n",
            "fppi:  97763.20987654322\n",
            "0.5\n",
            "sn:  0.9808762180787789\n",
            "fppi:  85151.83950617284\n",
            "0.6\n",
            "sn:  0.9739881305827571\n",
            "fppi:  72394.50617283951\n",
            "0.7\n",
            "sn:  0.9626368323303055\n",
            "fppi:  59131.567901234564\n",
            "0.8\n",
            "sn:  0.9414782104509493\n",
            "fppi:  44431.61728395062\n",
            "0.9\n",
            "sn:  0.892350447299412\n",
            "fppi:  26864.46913580247\n",
            "0.99\n",
            "sn:  0.5611847614316616\n",
            "fppi:  2681.543209876543\n",
            "0.999\n",
            "sn:  0.001640410645737987\n",
            "fppi:  13.061728395061728\n",
            "0.9999\n",
            "sn:  0.001640410645737987\n",
            "fppi:  13.061728395061728\n",
            "0.99999\n",
            "sn:  0.001640410645737987\n",
            "fppi:  13.061728395061728\n",
            "1.0\n",
            "sn:  0.0\n",
            "fppi:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnNDIjEh_iD3",
        "colab_type": "text"
      },
      "source": [
        "### Generate segmented output masks from the probability maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37-KEnaJ_lxg",
        "colab_type": "code",
        "outputId": "4cb1dd97-c2dd-4144-f29e-5576fc42da3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage.measurements import label\n",
        "from PIL import Image\n",
        "from scipy.special import expit\n",
        "import os, sys\n",
        "\n",
        "saver = tf.train.import_meta_graph(\"drive/My Drive/data/models/model.ckpt.meta\")\n",
        "sess = tf.InteractiveSession()\n",
        "saver.restore(sess, \"drive/My Drive/data/models/model.ckpt\")\n",
        "X, mode = tf.get_collection(\"inputs\")[:2]\n",
        "pred = tf.get_collection(\"outputs\")[0]\n",
        "\n",
        "def read_image(image_path, gray=False):\n",
        "    \"\"\"Returns an image array\n",
        "    Args:\n",
        "        image_path (str): Path to image.jpg\n",
        "    Returns:\n",
        "        3-D array: RGB numpy image array\n",
        "    \"\"\"\n",
        "    if gray:\n",
        "        return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    image = cv2.imread(image_path)    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def pipeline(image, image_WH=(512, 512)):\n",
        "    image = np.copy(image)\n",
        "    H, W, C = image.shape\n",
        "    \n",
        "    if (W, H) != image_WH:\n",
        "        image = cv2.resize(image, image_WH)\n",
        "    \n",
        "    mask_pred = sess.run(pred, feed_dict={X: np.expand_dims(image, 0),\n",
        "                                          mode: False})\n",
        "    \n",
        "    mask_pred = np.squeeze(mask_pred)\n",
        "    mask_pred = expit(mask_pred)\n",
        "    # mask_pred = mask_pred > threshold\n",
        "    return mask_pred\n",
        "\n",
        "output_dir = \"drive/My Drive/data/output/predicted/\"\n",
        "dir = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join(dir,output_dir)):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "for image_path in os.listdir(os.path.join(dir,\"drive/My Drive/data/output/prob/\")):\n",
        "    im_prob = Image.open(\"drive/My Drive/data/output/prob/\"+image_path)\n",
        "    arr_prob = (np.array(im_prob)).astype(float)/255\n",
        "    threshold = 0.8\n",
        "    arr_pred = (arr_prob > threshold).astype('uint8')\n",
        "    # image = read_image(\"test_data/\"+image_path)\n",
        "    # predicted_image = np.zeros((2848, 4288), dtype=float)\n",
        "\n",
        "    # for i in range(10):\n",
        "    #     for j in range(16):\n",
        "    #         top_y = i*256\n",
        "    #         if (i==9):\n",
        "    #             top_y = 2336\n",
        "    #         top_x = j*256\n",
        "    #         if (j==15):\n",
        "    #             top_x = 3776\n",
        "\n",
        "    #         image_crop = image[top_y:top_y+512, top_x:top_x+512]\n",
        "    #         predicted_crop = pipeline(image_crop)\n",
        "    #         predicted_image[top_y:top_y+512, top_x:top_x+512] = np.maximum(predicted_image[top_y:top_y+512, top_x:top_x+512], predicted_crop)\n",
        "    #(unique, counts) = np.unique(arr_pred*255, return_counts=True)\n",
        "    #frequencies = np.asarray((unique, counts)).T\n",
        "    #print(frequencies)\n",
        "    predicted_save = Image.fromarray(arr_pred*255)\n",
        "    predicted_save.save(output_dir+image_path, \"JPEG\", quality=100)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/My Drive/data/models/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjewf0kzHheN",
        "colab_type": "text"
      },
      "source": [
        "### Calculate sensitivity and precison values for individual images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfPnyrHXHm4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, sys\n",
        "\n",
        "training_size=80\n",
        "gt_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "pred_dir = \"drive/My Drive/data/output/predicted/\"\n",
        "sn = np.empty(training_size, dtype=float)\n",
        "ppv = np.empty(training_size, dtype=float)\n",
        "sp = np.empty(training_size, dtype=float)\n",
        "image_paths = np.empty(training_size, dtype=object)\n",
        "\n",
        "dir = os.getcwd()\n",
        "i=0\n",
        "for image_path in os.listdir(os.path.join(dir,gt_dir)):\n",
        "\timage_paths[i] = image_path\n",
        "\tim_gt = Image.open(gt_dir+image_path)\n",
        "\timg = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "\tim_pred = Image.open(pred_dir+img)\n",
        "\tarr_gt = np.array(im_gt)\n",
        "\tarr_pred = np.array(im_pred)\n",
        "\tarr_pred = arr_pred > 0\n",
        "\t#(unique, counts) = np.unique(arr_gt, return_counts=True)\n",
        "\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t#print(\"arr_gt: \",frequencies)\n",
        "\t#(unique, counts) = np.unique(arr_pred, return_counts=True)\n",
        "\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t#print(\"arr_pred: \",frequencies)\n",
        "\ttrue_p = np.sum(np.logical_and(arr_gt, arr_pred))\n",
        "\tactual_p = np.sum(arr_gt)\n",
        "\tpred_p = np.sum(arr_pred)\n",
        "\t\n",
        "\tfalse_p = pred_p - true_p\n",
        "\tactual_n = 4288*2848 - actual_p\n",
        "\ttrue_n = actual_n - false_p\n",
        "\t#print (\"True pos: \", true_p)\n",
        "\t#print (\"Actual pos: \", actual_p)\n",
        "\t#print (\"Pred pos: \", pred_p)\n",
        "\tif actual_p == 0:\n",
        "\t\tsn[i] = 1\n",
        "\telse:\n",
        "\t\tsn[i] = float(true_p)/float(actual_p)\n",
        "\tif pred_p == 0:\n",
        "\t\tppv[i] = 1\n",
        "\telse:\n",
        "\t\tppv[i] = float(true_p)/float(pred_p)\n",
        "\t#print (i)\n",
        "\tif actual_n == 0:\n",
        "\t\tsp[i] = 1\n",
        "\telse:\n",
        "\t\tsp[i] = float(true_n)/float(actual_n)\n",
        "\ti+=1\n",
        "\n",
        "sn_csv = np.stack((image_paths,sn), axis=1)\n",
        "ppv_csv = np.stack((image_paths,ppv), axis=1)\n",
        "sp_csv = np.stack((image_paths,sp), axis=1)\n",
        "\n",
        "np.savetxt(\"drive/My Drive/data/sn.csv\", sn_csv, delimiter=\",\", fmt=\"%s\")\n",
        "np.savetxt(\"drive/My Drive/data/ppv.csv\", ppv_csv, delimiter=\",\", fmt=\"%s\")\n",
        "np.savetxt(\"drive/My Drive/data/sp.csv\", sp_csv, delimiter=\",\", fmt=\"%s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hwL8cLfH89I",
        "colab_type": "text"
      },
      "source": [
        "### compute average statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJaPdfF89P7x",
        "colab_type": "code",
        "outputId": "3cca101e-3813-4268-caea-a2efd1bdca76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "#np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "gt_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "pred_dir = \"drive/My Drive/data/output/predicted/\"\n",
        "true_p=0\n",
        "actual_p=0\n",
        "pred_p=0\n",
        "false_p=0\n",
        "false_n=0\n",
        "actual_n=0\n",
        "true_n=0\n",
        "pred_n=0\n",
        "\n",
        "dir = os.getcwd()\n",
        "\n",
        "for image_path in os.listdir(os.path.join(dir,gt_dir)):\n",
        "  im_gt = Image.open(gt_dir+image_path)\n",
        "  img = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "  im_pred = Image.open(pred_dir+img)\n",
        "  arr_gt = np.array(im_gt)\n",
        "  arr_pred = np.array(im_pred)\n",
        "  arr_pred = arr_pred > 0\n",
        "  #(unique, counts) = np.unique(arr_pred, return_counts=True)\n",
        "  #frequencies = np.asarray((unique, counts)).T\n",
        "  #print(\"arr_pred: \",frequencies)\n",
        "\n",
        "  tp = np.sum(np.logical_and(arr_gt, arr_pred))\n",
        "  #print(\"tp: \",tp)\n",
        "  ap = np.sum(arr_gt)\n",
        "  #print(\"ap: \",ap)\n",
        "  pp = np.sum(arr_pred)\n",
        "\n",
        "  fp = pp - tp\n",
        "  an = 4288*2848 - ap\n",
        "  pn = 4288*2848 - pp\n",
        "  tn = an - fp\n",
        "  fn = pn - tn\n",
        "\n",
        "  true_p += tp\n",
        "  actual_p += ap\n",
        "  pred_p += pp\n",
        "  pred_n += pn\n",
        "  false_p += fp\n",
        "  actual_n += an\n",
        "  true_n += tn\n",
        "  false_n +=fn\n",
        "\n",
        "\n",
        "sn = float(true_p)/float(actual_p)\n",
        "ppv = float(true_p)/float(pred_p)\n",
        "sp = float(true_n)/float(actual_n)\n",
        "npv = float(true_n)/float(pred_n)\n",
        "acc = float(true_p + true_n)/float(actual_p + actual_n)\n",
        "f1 = float(2*true_p)/float((2*true_p)+false_p+false_n)\n",
        "\n",
        "print (\"Sensitivity/Recall/True Positive Rate(TPR)(TP/P): \", sn)\n",
        "print (\"Precision/Positive Predictive Value(PPV)(TP/TP+FP): \", ppv)\n",
        "print (\"Specificity/Selectivity/True Negative Rate(TNR)(TN/N): \", sp)\n",
        "print (\"Negative Predictive Value(NPV)(TN/TN+FN): \", npv)\n",
        "print (\"Accuracy(TP+TN/P+N)\",acc)\n",
        "print (\"F1 Score(2TP/2TP+FP+FN)\",f1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitivity/Recall/True Positive Rate(TPR)(TP/P):  0.9425759438650518\n",
            "Precision/Positive Predictive Value(PPV)(TP/TP+FP):  0.7188802057557396\n",
            "Specificity/Selectivity/True Negative Rate(TNR)(TN/N):  0.9961816109225247\n",
            "Negative Predictive Value(NPV)(TN/TN+FN):  0.9994032052285645\n",
            "Accuracy(TP+TN/P+N) 0.9956319893084176\n",
            "F1 Score(2TP/2TP+FP+FN) 0.8156690606860414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9LjYv56IQ7Q",
        "colab_type": "text"
      },
      "source": [
        "### precision score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ott8N-YVISa9",
        "colab_type": "code",
        "outputId": "6ae66a0c-f6bc-439a-8aea-3faa125b7c53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, sys\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "training_size=80\n",
        "gt_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "prob_dir = \"drive/My Drive/data/output/prob/\"\n",
        "\n",
        "dir = os.getcwd()\n",
        "\n",
        "i=0\n",
        "sum_pav=0\n",
        "for image_path in os.listdir(os.path.join(dir,gt_dir)):\n",
        "\t# print image_path\n",
        "\tim_gt = Image.open(gt_dir+image_path)\n",
        "\timg = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "\tim_prob = Image.open(prob_dir+img)\n",
        "\tarr_gt = (np.array(im_gt)).astype(bool)\n",
        "\tarr_prob = (np.array(im_prob)).astype(float)/255\n",
        "\t#(unique, counts) = np.unique(arr_gt, return_counts=True)\n",
        "\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t#print(\"arr_prob: \",frequencies)\n",
        "\tpav = average_precision_score(arr_gt.reshape((-1)),arr_prob.reshape((-1)))\n",
        "\tsum_pav = sum_pav+pav\n",
        "\ti = i+1\n",
        "\n",
        "mpav = sum_pav/i\n",
        "print(mpav)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7908225819055167\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}