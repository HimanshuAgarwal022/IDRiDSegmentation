{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDRiDseg.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EeZa_92kuQag",
        "yrfnYhrjuYYQ",
        "BklkTaFzut95",
        "cHjVOXKMfXSa",
        "p2JGpoMrtNx3",
        "ByIpNhUk-xQr",
        "rnNDIjEh_iD3",
        "8hwL8cLfH89I",
        "r9LjYv56IQ7Q"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPv0yGo6469WpDfP9n9KdfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimanshuAgarwal022/IDRiDSegmentation/blob/master/IDRiDseg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcSma7_RfLnk",
        "colab_type": "text"
      },
      "source": [
        "### load drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf1EOC0CtWWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeZa_92kuQag",
        "colab_type": "text"
      },
      "source": [
        "### convert ground truths to binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi6Fax6vuOoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "#from resizeimage import resizeimage\n",
        "import os, sys\n",
        "\n",
        "def cmp(a, b):\n",
        "    return (a > b) - (a < b) \n",
        "\n",
        "def resizeImage(infile,file, output_dir, size=(4288,2848)):\n",
        "  outfile = os.path.splitext(file)[0]\n",
        "  extension = os.path.splitext(file)[1]\n",
        "  #print(outfile)\n",
        "  #print(extension)\n",
        "  #print(infile)\n",
        "  #if (cmp(extension, \".jpg\")):\n",
        "    #print(\"dsd\")\n",
        "    #return\n",
        "\n",
        "  if infile != outfile:\n",
        "    try :\n",
        "      im = Image.open(infile)\n",
        "      gray = im.convert('L')\n",
        "      bw = gray.point(lambda x: 0 if x<50 else 255, '1')\n",
        "      # im = resizeimage.resize_cover(im, [960, 640])\n",
        "      bw.save(output_dir+outfile[:-3]+extension,\"TIFF\",quality=100)\n",
        "      #print(\"sucess\")\n",
        "    #except IOError:\n",
        "    #  print (\"cannot reduce image for \", infile)\n",
        "    except e:\n",
        "      print (e)\n",
        "    \n",
        "\n",
        "\n",
        "output_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "annot_dir = \"drive/My Drive/data/annotations/\"\n",
        "dir = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join(dir,output_dir)):\n",
        "  os.mkdir(output_dir)\n",
        "annot = os.path.join(dir,annot_dir)\n",
        "for file in os.listdir(annot):\n",
        "  #print(file)\n",
        "  resizeImage(os.path.join(annot,file),file,output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrfnYhrjuYYQ",
        "colab_type": "text"
      },
      "source": [
        "### extract patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BhM3OxTuiWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "#from resizeimage import resizeimage\n",
        "import os, sys\n",
        "import numpy as np\n",
        "\n",
        "dir = os.getcwd()\n",
        "output_dir_data = \"drive/My Drive/data/output/patches/\"\n",
        "output_dir_mask = \"drive/My Drive/data/output/labels/\"\n",
        "if not os.path.exists(os.path.join(dir,output_dir_data)):\n",
        "    os.mkdir(output_dir_data)\n",
        "if not os.path.exists(os.path.join(dir,output_dir_mask)):\n",
        "    os.mkdir(output_dir_mask)\n",
        "\n",
        "dir_data = os.path.join(dir,\"drive/My Drive/data/output/images/\")\n",
        "dir_mask = os.path.join(dir,\"drive/My Drive/data/output/annotations/\")\n",
        "\n",
        "# im = Image.open(os.path.join(dir_mask,\"IDRiD_06.tif\"))\n",
        "# im_crop = im.crop((2000,0,2000+512,0+256))\n",
        "# im_crop.show()\n",
        "# image_np = np.array(im_crop)\n",
        "# print np.sum(image_np)\n",
        "\n",
        "negative_patches = []\n",
        "positive_count = 0\n",
        "\n",
        "for file in os.listdir(dir_mask):\n",
        "    outfile = os.path.splitext(file)[0]\n",
        "    extension = os.path.splitext(file)[1]\n",
        "    #if (cmp(extension, \".jpg\")):\n",
        "    #    continue\n",
        "    img = outfile + \".jpg\"\n",
        "    im = Image.open(os.path.join(dir_mask,file))\n",
        "    imd = Image.open(os.path.join(dir_data,img))\n",
        "    # image_np = np.array(im)\n",
        "    # print np.sum([True, True])\n",
        "    # im_crop = im.crop((1900,0,1900+512,0+512))\n",
        "    patch_id = 0\n",
        "    for i in range(6): #10\n",
        "    \tfor j in range(9): #16\n",
        "            top_y = i*512 #256\n",
        "            if (i==5): #9\n",
        "                top_y = 2336\n",
        "            top_x = j*512 #256\n",
        "            if (j==8): #15\n",
        "                top_x = 3776\n",
        "\n",
        "            im_crop = im.crop((top_x,top_y,top_x+512,top_y+512))\n",
        "            imd_crop = imd.crop((top_x,top_y,top_x+512,top_y+512))\n",
        "            im_crop.save(output_dir_mask+outfile+\"_p\"+str(patch_id)+extension,\"JPEG\",quality=100)\n",
        "            imd_crop.save(output_dir_data+outfile+\"_p\"+str(patch_id)+extension,\"JPEG\",quality=100)\n",
        "            if (np.sum(np.array(im_crop)) < 100):\n",
        "                negative_patches.append(output_dir_mask+outfile+\"_p\"+str(patch_id)+extension)\n",
        "            else:\n",
        "                positive_count += 1\n",
        "\n",
        "            patch_id += 1\n",
        "\n",
        "negative_patches = np.array(negative_patches)\n",
        "# np.savetxt(\"negative.csv\", negative_patches, delimiter=\",\", fmt=\"%s\")\n",
        "\n",
        "negative_count = negative_patches.size\n",
        "delete_count = negative_count - 4*positive_count\n",
        "np.random.shuffle(negative_patches)\n",
        "split_idx = delete_count\n",
        "delete_patches = negative_patches[:split_idx]\n",
        "\n",
        "for idx in range(delete_patches.size):\n",
        "    os.remove(delete_patches[idx])\n",
        "    os.remove(os.path.join(output_dir_data,delete_patches[idx][34:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BklkTaFzut95",
        "colab_type": "text"
      },
      "source": [
        "### split in train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krh5Z4OHuvBu",
        "colab_type": "code",
        "outputId": "578ba777-25f8-42df-dd85-1de9324606fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "dir = \"drive/My Drive/data/output/patches/\"\n",
        "#dir = \"drive/My Drive/data/output/images/\"\n",
        "image_paths = os.listdir(dir)\n",
        "\n",
        "length = len(image_paths)\n",
        "img_paths = np.empty(length, dtype=object)\n",
        "\n",
        "i=0\n",
        "for file in image_paths:\n",
        "\timg_paths[i] = \"drive/My Drive/data/output/patches/\" + file\n",
        "\t#img_paths[i] = \"drive/My Drive/data/output/images/\" + file\n",
        "\t#print (img_paths[i])\n",
        "\ti+=1\n",
        "\n",
        "# print (img_paths)\n",
        "np.random.shuffle(img_paths)\n",
        "#split_idx = int(img_paths.shape[0] * 1)\n",
        "split_idx = 2670\n",
        "train_paths = img_paths[:split_idx]\n",
        "test_paths = img_paths[split_idx:]\n",
        "\n",
        "train_paths_ = np.copy(train_paths)\n",
        "test_paths_ = np.copy(test_paths)\n",
        "print(\"train set: \",train_paths.size)\n",
        "print(\"test set: \",test_paths.size)\n",
        "for i in range(train_paths.size):\n",
        "\ttrain_paths_[i] = train_paths[i][35:]\n",
        "\t#train_paths_[i] = train_paths[i][34:]\n",
        "\ttrain_paths_[i] = \"drive/My Drive/data/output/labels/\" + train_paths_[i]\n",
        "\t#train_paths_[i] = \"drive/My Drive/data/output/annotations/\" + train_paths_[i]\n",
        "\t#print (train_paths_[i])\n",
        "\n",
        "#print (\"split\")\n",
        "\n",
        "for i in range(test_paths.size):\n",
        "\ttest_paths_[i] = test_paths[i][35:]\n",
        "\t#test_paths_[i] = test_paths[i][34:]\n",
        "\ttest_paths_[i] = \"drive/My Drive/data/output/labels/\" + test_paths_[i]\n",
        "\t#test_paths_[i] = \"drive/My Drive/data/output/annotations/\" + test_paths_[i]\n",
        "\t#print (test_paths_[i])\n",
        "\n",
        "train_csv = np.stack((train_paths,train_paths_), axis=1)\n",
        "test_csv = np.stack((test_paths,test_paths_), axis=1)\n",
        "\n",
        "np.savetxt(\"train.csv\", train_csv, delimiter=\",\", fmt=\"%s\")\n",
        "np.savetxt(\"test.csv\", test_csv, delimiter=\",\", fmt=\"%s\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train set:  2670\n",
            "test set:  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjVOXKMfXSa",
        "colab_type": "text"
      },
      "source": [
        "### load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YpGHNnLuAt9",
        "colab_type": "code",
        "outputId": "92310c7a-3dce-4066-bd58-8f6080af606a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#import os\n",
        "#os.getcwd()\n",
        "#os.listdir()\n",
        "#os.path.exists('drive/My Drive/')\n",
        "#for roots,dirs,files in os.walk('drive/My Drive'):               \n",
        "#                    print(roots,dirs,files)\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "TensorFlow Version: 1.15.2\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLtpXRru2kh",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIVvZ3JxsepC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm drive/My Drive/data/models -rf\n",
        "#!rm drive/My Drive/data/logs -rf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jHwMuhTxgBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def image_augmentation(image, mask):\n",
        "    \"\"\"Returns (maybe) augmented images\n",
        "    (1) Random flip (left <--> right)\n",
        "    (2) Random flip (up <--> down)\n",
        "    (3) Random brightness\n",
        "    (4) Random hue\n",
        "    Args:\n",
        "        image (3-D Tensor): Image tensor of (H, W, C)\n",
        "        mask (3-D Tensor): Mask image tensor of (H, W, 1)\n",
        "    Returns:\n",
        "        image: Maybe augmented image (same shape as input `image`)\n",
        "        mask: Maybe augmented mask (same shape as input `mask`)\n",
        "    \"\"\"\n",
        "    concat_image = tf.concat([image, mask], axis=-1)\n",
        "\n",
        "    maybe_flipped = tf.image.random_flip_left_right(concat_image)\n",
        "    maybe_flipped = tf.image.random_flip_up_down(concat_image)\n",
        "\n",
        "    image = maybe_flipped[:, :, :-1]\n",
        "    mask = maybe_flipped[:, :, -1:]\n",
        "\n",
        "    image = tf.image.random_brightness(image, 0.7)\n",
        "    image = tf.image.random_hue(image, 0.3)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def get_image_mask(queue, augmentation=True):\n",
        "    \"\"\"Returns `image` and `mask`\n",
        "    Input pipeline:\n",
        "        Queue -> CSV -> FileRead -> Decode JPEG\n",
        "    (1) Queue contains a CSV filename\n",
        "    (2) Text Reader opens the CSV\n",
        "        CSV file contains two columns\n",
        "        [\"path/to/image.jpg\", \"path/to/mask.jpg\"]\n",
        "    (3) File Reader opens both files\n",
        "    (4) Decode JPEG to tensors\n",
        "    Notes:\n",
        "        height, width = 640, 960\n",
        "    Returns\n",
        "        image (3-D Tensor): (640, 960, 3)\n",
        "        mask (3-D Tensor): (640, 960, 1)\n",
        "    \"\"\"\n",
        "    text_reader = tf.TextLineReader(skip_header_lines=1)\n",
        "    _, csv_content = text_reader.read(queue)\n",
        "\n",
        "    image_path, mask_path = tf.decode_csv(\n",
        "        csv_content, record_defaults=[[\"\"], [\"\"]])\n",
        "\n",
        "    image_file = tf.read_file(image_path)\n",
        "    mask_file = tf.read_file(mask_path)\n",
        "\n",
        "    image = tf.image.decode_jpeg(image_file, channels=3)\n",
        "    image.set_shape([512, 512, 3])\n",
        "    image = tf.cast(image, tf.float32)\n",
        "\n",
        "    mask = tf.image.decode_jpeg(mask_file, channels=1)\n",
        "    mask.set_shape([512, 512, 1])\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "    mask = mask / (tf.reduce_max(mask) + 1e-7)\n",
        "\n",
        "    if augmentation:\n",
        "        image, mask = image_augmentation(image, mask)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def conv_conv_pool(input_,\n",
        "                   n_filters,\n",
        "                   training,\n",
        "                   flags,\n",
        "                   name,\n",
        "                   pool=True,\n",
        "                   activation=tf.nn.relu):\n",
        "    \"\"\"{Conv -> BN -> RELU}x2 -> {Pool, optional}\n",
        "    Args:\n",
        "        input_ (4-D Tensor): (batch_size, H, W, C)\n",
        "        n_filters (list): number of filters [int, int]\n",
        "        training (1-D Tensor): Boolean Tensor\n",
        "        name (str): name postfix\n",
        "        pool (bool): If True, MaxPool2D\n",
        "        activation: Activaion functions\n",
        "    Returns:\n",
        "        net: output of the Convolution operations\n",
        "        pool (optional): output of the max pooling operations\n",
        "    \"\"\"\n",
        "    net = input_\n",
        "\n",
        "    with tf.variable_scope(\"layer{}\".format(name)):\n",
        "        for i, F in enumerate(n_filters):\n",
        "            net = tf.layers.conv2d(\n",
        "                net,\n",
        "                F, (3, 3),\n",
        "                activation=None,\n",
        "                padding='same',\n",
        "                kernel_regularizer=tf.contrib.layers.l2_regularizer(flags.reg),\n",
        "                name=\"conv_{}\".format(i + 1))\n",
        "            net = tf.layers.batch_normalization(\n",
        "                net, training=training, name=\"bn_{}\".format(i + 1))\n",
        "            net = activation(net, name=\"relu{}_{}\".format(name, i + 1))\n",
        "\n",
        "        if pool is False:\n",
        "            return net\n",
        "\n",
        "        pool = tf.layers.max_pooling2d(\n",
        "            net, (2, 2), strides=(2, 2), name=\"pool_{}\".format(name))\n",
        "\n",
        "        return net, pool\n",
        "\n",
        "\n",
        "def upconv_concat(inputA, input_B, n_filter, flags, name):\n",
        "    \"\"\"Upsample `inputA` and concat with `input_B`\n",
        "    Args:\n",
        "        input_A (4-D Tensor): (N, H, W, C)\n",
        "        input_B (4-D Tensor): (N, 2*H, 2*H, C2)\n",
        "        name (str): name of the concat operation\n",
        "    Returns:\n",
        "        output (4-D Tensor): (N, 2*H, 2*W, C + C2)\n",
        "    \"\"\"\n",
        "    up_conv = upconv_2D(inputA, n_filter, flags, name)\n",
        "\n",
        "    return tf.concat(\n",
        "        [up_conv, input_B], axis=-1, name=\"concat_{}\".format(name))\n",
        "\n",
        "\n",
        "def upconv_2D(tensor, n_filter, flags, name):\n",
        "    \"\"\"Up Convolution `tensor` by 2 times\n",
        "    Args:\n",
        "        tensor (4-D Tensor): (N, H, W, C)\n",
        "        n_filter (int): Filter Size\n",
        "        name (str): name of upsampling operations\n",
        "    Returns:\n",
        "        output (4-D Tensor): (N, 2 * H, 2 * W, C)\n",
        "    \"\"\"\n",
        "\n",
        "    return tf.layers.conv2d_transpose(\n",
        "        tensor,\n",
        "        filters=n_filter,\n",
        "        kernel_size=2,\n",
        "        strides=2,\n",
        "        kernel_regularizer=tf.contrib.layers.l2_regularizer(flags.reg),\n",
        "        name=\"upsample_{}\".format(name))\n",
        "\n",
        "\n",
        "def make_unet(X, training, flags=None):\n",
        "    \"\"\"Build a U-Net architecture\n",
        "    Args:\n",
        "        X (4-D Tensor): (N, H, W, C)\n",
        "        training (1-D Tensor): Boolean Tensor is required for batchnormalization layers\n",
        "    Returns:\n",
        "        output (4-D Tensor): (N, H, W, C)\n",
        "            Same shape as the `input` tensor\n",
        "    Notes:\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "    \"\"\"\n",
        "    net = X / 127.5 - 1\n",
        "    conv1, pool1 = conv_conv_pool(net, [16, 16], training, flags, name=1)\n",
        "    conv2, pool2 = conv_conv_pool(pool1, [32, 32], training, flags, name=2)\n",
        "    conv3, pool3 = conv_conv_pool(pool2, [64, 64], training, flags, name=3)\n",
        "    conv4, pool4 = conv_conv_pool(pool3, [128, 128], training, flags, name=4)\n",
        "    conv5, pool5 = conv_conv_pool(pool4, [256, 256], training, flags, name=5)\n",
        "    conv6, pool6 = conv_conv_pool(pool5, [512, 512], training, flags, name=6)\n",
        "    conv7 = conv_conv_pool(\n",
        "        pool6, [1024, 1024], training, flags, name=7, pool=False)\n",
        "\n",
        "    up8 = upconv_concat(conv7, conv6, 512, flags, name=8)\n",
        "    conv8 = conv_conv_pool(up8, [512, 512], training, flags, name=8, pool=False)\n",
        "\n",
        "    up9 = upconv_concat(conv8, conv5, 256, flags, name=9)\n",
        "    conv9 = conv_conv_pool(up9, [256, 256], training, flags, name=9, pool=False)\n",
        "\n",
        "    up10 = upconv_concat(conv9, conv4, 128, flags, name=10)\n",
        "    conv10 = conv_conv_pool(up10, [128, 128], training, flags, name=10, pool=False)\n",
        "\n",
        "    up11 = upconv_concat(conv10, conv3, 64, flags, name=11)\n",
        "    conv11 = conv_conv_pool(up11, [64, 64], training, flags, name=11, pool=False)\n",
        "\n",
        "    up12 = upconv_concat(conv11, conv2, 32, flags, name=12)\n",
        "    conv12 = conv_conv_pool(up12, [32, 32], training, flags, name=12, pool=False)\n",
        "\n",
        "    up13 = upconv_concat(conv12, conv1, 16, flags, name=13)\n",
        "    conv13 = conv_conv_pool(up13, [16, 16], training, flags, name=13, pool=False)\n",
        "\n",
        "    # return tf.layers.conv2d(\n",
        "    #     conv13,\n",
        "    #     1, (1, 1),\n",
        "    #     name='final',\n",
        "    #     activation=tf.nn.sigmoid,\n",
        "    #     padding='same')\n",
        "    return tf.layers.conv2d(\n",
        "        conv13,\n",
        "        1, (1, 1),\n",
        "        name='final',\n",
        "        activation=None,\n",
        "        padding='same')\n",
        "\n",
        "def BCE_(y_pred, y_true):\n",
        "    # weight ratio = 9:1\n",
        "    # 9-1=8\n",
        "    class_weights = tf.constant([8],dtype=tf.float32)\n",
        "    tensor_one = tf.constant([1],dtype=tf.float32)\n",
        "\n",
        "    pred_flat = tf.reshape(y_pred, [-1, 1])\n",
        "    true_flat = tf.reshape(y_true, [-1, 1])\n",
        "\n",
        "    weight_map = tf.multiply(true_flat, class_weights)\n",
        "    weight_map = tf.add(weight_map, tensor_one)\n",
        "\n",
        "    loss_map = tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_flat, labels=true_flat)\n",
        "    loss_map = tf.multiply(loss_map, weight_map)\n",
        "    loss = tf.reduce_mean(loss_map)\n",
        "    return loss\n",
        "\n",
        "def IOU_(y_pred, y_true):\n",
        "    \"\"\"Returns a (approx) IOU score\n",
        "    intesection = y_pred.flatten() * y_true.flatten()\n",
        "    Then, IOU = 2 * intersection / (y_pred.sum() + y_true.sum() + 1e-7) + 1e-7\n",
        "    Args:\n",
        "        y_pred (4-D array): (N, H, W, 1)\n",
        "        y_true (4-D array): (N, H, W, 1)\n",
        "    Returns:\n",
        "        float: IOU score\n",
        "    \"\"\"\n",
        "    H, W, _ = y_pred.get_shape().as_list()[1:]\n",
        "\n",
        "    pred_flat = tf.reshape(y_pred, [-1, H * W])\n",
        "    true_flat = tf.reshape(y_true, [-1, H * W])\n",
        "\n",
        "    intersection = 2 * tf.reduce_sum(pred_flat * true_flat, axis=1) + 1e-7\n",
        "    denominator = tf.reduce_sum(pred_flat, axis=1) + tf.reduce_sum(true_flat, axis=1) + 1e-7\n",
        "\n",
        "    return tf.reduce_mean(intersection / denominator)\n",
        "\n",
        "\n",
        "def make_train_op(y_pred, y_true):\n",
        "    \"\"\"Returns a training operation\n",
        "    Args:\n",
        "        y_pred (4-D Tensor): (N, H, W, 1)\n",
        "        y_true (4-D Tensor): (N, H, W, 1)\n",
        "    Returns:\n",
        "        train_op: minimize operation\n",
        "    \"\"\"\n",
        "    # loss = -IOU_(y_pred, y_true)\n",
        "    loss = BCE_(y_pred, y_true)\n",
        "\n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "    # optim = tf.train.AdamOptimizer()\n",
        "    optim = tf.train.AdamOptimizer(1e-4)\n",
        "    return optim.minimize(loss, global_step=global_step)\n",
        "class flags:\n",
        "  epochs = 100\n",
        "  batch_size = 8\n",
        "  logdir = \"drive/My Drive/data/logs/\"\n",
        "  reg = 0.1\n",
        "  ckdir = \"drive/My Drive/data/models/\"\n",
        "\n",
        "\n",
        "'''def read_flags():\n",
        "    \"\"\"Returns flags\"\"\"\n",
        "\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", default=1, type=int, help=\"Number of epochs\")\n",
        "\n",
        "    parser.add_argument(\"--batch-size\", default=8, type=int, help=\"Batch size\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--logdir\", default=\"logdir\", help=\"Tensorboard log directory\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--reg\", type=float, default=0.1, help=\"L2 Regularizer Term\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--ckdir\", default=\"models\", help=\"Checkpoint directory\")\n",
        "\n",
        "    flags = parser.parse_args()\n",
        "    return flags'''\n",
        "\n",
        "\n",
        "def main():\n",
        "    train = pd.read_csv(\"./train.csv\")\n",
        "    n_train = train.shape[0]\n",
        "\n",
        "    test = pd.read_csv(\"./test.csv\")\n",
        "    n_test = test.shape[0]\n",
        "\n",
        "    current_time = time.strftime(\"%m/%d/%H/%M/%S\")\n",
        "    train_logdir = os.path.join(flags.logdir, \"train\", current_time)\n",
        "    test_logdir = os.path.join(flags.logdir, \"test\", current_time)\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    X = tf.placeholder(tf.float32, shape=[None, 512, 512, 3], name=\"X\")\n",
        "    y = tf.placeholder(tf.float32, shape=[None, 512, 512, 1], name=\"y\")\n",
        "    mode = tf.placeholder(tf.bool, name=\"mode\")\n",
        "\n",
        "    pred = make_unet(X, mode, flags)\n",
        "\n",
        "    tf.add_to_collection(\"inputs\", X)\n",
        "    tf.add_to_collection(\"inputs\", mode)\n",
        "    tf.add_to_collection(\"outputs\", pred)\n",
        "\n",
        "    tf.summary.histogram(\"Predicted Mask\", pred)\n",
        "    tf.summary.image(\"Predicted Mask\", pred)\n",
        "\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        train_op = make_train_op(pred, y)\n",
        "\n",
        "    IOU_op = IOU_(pred, y)\n",
        "    IOU_op = tf.Print(IOU_op, [IOU_op])\n",
        "    tf.summary.scalar(\"IOU\", IOU_op)\n",
        "\n",
        "    train_csv = tf.train.string_input_producer(['train.csv'])\n",
        "    test_csv = tf.train.string_input_producer(['test.csv'])\n",
        "    train_image, train_mask = get_image_mask(train_csv)\n",
        "    test_image, test_mask = get_image_mask(test_csv, augmentation=False)\n",
        "\n",
        "    X_batch_op, y_batch_op = tf.train.shuffle_batch(\n",
        "        [train_image, train_mask],\n",
        "        batch_size=flags.batch_size,\n",
        "        capacity=flags.batch_size * 5,\n",
        "        min_after_dequeue=flags.batch_size * 2,\n",
        "        allow_smaller_final_batch=True)\n",
        "\n",
        "    X_test_op, y_test_op = tf.train.batch(\n",
        "        [test_image, test_mask],\n",
        "        batch_size=flags.batch_size,\n",
        "        capacity=flags.batch_size * 2,\n",
        "        allow_smaller_final_batch=True)\n",
        "\n",
        "    summary_op = tf.summary.merge_all()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        train_summary_writer = tf.summary.FileWriter(train_logdir, sess.graph)\n",
        "        test_summary_writer = tf.summary.FileWriter(test_logdir)\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        if os.path.exists(flags.ckdir) and tf.train.checkpoint_exists(\n",
        "                flags.ckdir):\n",
        "            latest_check_point = tf.train.latest_checkpoint(flags.ckdir)\n",
        "            saver.restore(sess, latest_check_point)\n",
        "\n",
        "        else:\n",
        "            #try:\n",
        "            #    os.rmdir(flags.ckdir)\n",
        "            #except IOError:\n",
        "            #    pass\n",
        "            os.mkdir(flags.ckdir)\n",
        "\n",
        "        try:\n",
        "            global_step = tf.train.get_global_step(sess.graph)\n",
        "\n",
        "            coord = tf.train.Coordinator()\n",
        "            threads = tf.train.start_queue_runners(coord=coord)\n",
        "            start = time.time()\n",
        "            for epoch in range(flags.epochs):\n",
        "                print(\"%d epochs completed in %fs\" %(epoch, time.time()-start))\n",
        "\n",
        "                for step in range(0, n_train, flags.batch_size):\n",
        "                    print(\"%d train steps in %fs\" %(step, time.time()-start))\n",
        "                    X_batch, y_batch = sess.run([X_batch_op, y_batch_op])\n",
        "\n",
        "                    _, step_iou, step_summary, global_step_value = sess.run(\n",
        "                        [train_op, IOU_op, summary_op, global_step],\n",
        "                        feed_dict={X: X_batch,\n",
        "                                   y: y_batch,\n",
        "                                   mode: True})\n",
        "\n",
        "                    train_summary_writer.add_summary(step_summary,\n",
        "                                                     global_step_value)\n",
        "\n",
        "                total_iou = 0\n",
        "                for step in range(0, n_test, flags.batch_size):\n",
        "                    print(\"%d test steps in %fs\" %(step, time.time()-start))\n",
        "                    X_test, y_test = sess.run([X_test_op, y_test_op])\n",
        "                    step_iou, step_summary = sess.run(\n",
        "                        [IOU_op, summary_op],\n",
        "                        feed_dict={X: X_test,\n",
        "                                   y: y_test,\n",
        "                                   mode: False})\n",
        "\n",
        "                    total_iou += step_iou * X_test.shape[0]\n",
        "\n",
        "                    test_summary_writer.add_summary(step_summary,\n",
        "                                                    (epoch + 1) * (step + 1))\n",
        "\n",
        "            saver.save(sess, \"{}/model.ckpt\".format(flags.ckdir))\n",
        "\n",
        "        finally:\n",
        "            coord.request_stop()\n",
        "            coord.join(threads)\n",
        "            saver.save(sess, \"{}/model.ckpt\".format(flags.ckdir))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #flags = read_flags()\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQOF55DOjN_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir 'drive/My Drive/data/logs'\n",
        "#%reload_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2JGpoMrtNx3",
        "colab_type": "text"
      },
      "source": [
        "### Test on a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXJmE8wYtUJ2",
        "colab_type": "code",
        "outputId": "440c7261-7d53-4334-8b35-a85b1c03542b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage.measurements import label\n",
        "from PIL import Image\n",
        "from scipy.special import expit\n",
        "\n",
        "saver = tf.train.import_meta_graph(\"drive/My Drive/data/models/model.ckpt.meta\")\n",
        "sess = tf.InteractiveSession()\n",
        "saver.restore(sess, \"drive/My Drive/data/models/model.ckpt\")\n",
        "X, mode = tf.get_collection(\"inputs\")[:2]\n",
        "pred = tf.get_collection(\"outputs\")[0]\n",
        "\n",
        "def read_image(image_path, gray=False):\n",
        "    \"\"\"Returns an image array\n",
        "    Args:\n",
        "        image_path (str): Path to image.jpg\n",
        "    Returns:\n",
        "        3-D array: RGB numpy image array\n",
        "    \"\"\"\n",
        "    if gray:\n",
        "        return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    image = cv2.imread(image_path)    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def pipeline(image, threshold=0.5, image_WH=(512, 512)):\n",
        "    image = np.copy(image)\n",
        "    H, W, C = image.shape\n",
        "    \n",
        "    if (W, H) != image_WH:\n",
        "        image = cv2.resize(image, image_WH)\n",
        "    \n",
        "    mask_pred = sess.run(pred, feed_dict={X: np.expand_dims(image, 0),\n",
        "                                          mode: False})\n",
        "    \n",
        "    mask_pred = np.squeeze(mask_pred)\n",
        "    mask_pred = expit(mask_pred)\n",
        "    # mask_pred = mask_pred > threshold\n",
        "    return mask_pred\n",
        "\n",
        "image_path = \"drive/My Drive/data/output/images/IDRiD_17.jpg\"\n",
        "image = read_image(image_path)\n",
        "predicted_image = np.zeros((2848, 4288), dtype=float)\n",
        "\n",
        "for i in range(6):\n",
        "\tfor j in range(9):\n",
        "\t\ttop_y = i*512\n",
        "\t\tif (i==5):\n",
        "\t\t\ttop_y = 2336\n",
        "\t\ttop_x = j*512\n",
        "\t\tif (j==8):\n",
        "\t\t\ttop_x = 3776\n",
        "\n",
        "\t\timage_crop = image[top_y:top_y+512, top_x:top_x+512]\n",
        "\t\tpredicted_crop = pipeline(image_crop)\n",
        "\t\tpredicted_image[top_y:top_y+512, top_x:top_x+512] = np.maximum(predicted_image[top_y:top_y+512, top_x:top_x+512], predicted_crop)\n",
        "\n",
        "threshold = 0.7\n",
        "predicted_image = predicted_image > threshold\n",
        "(unique, counts) = np.unique(predicted_image.astype('uint8')*255, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print(frequencies)\n",
        "\n",
        "predicted_save = Image.fromarray((predicted_image.astype('uint8'))*255)\n",
        "predicted_save.save(\"test_predicted.jpg\", \"JPEG\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/queue_runner_impl.py:391: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/data/models/model.ckpt\n",
            "[[       0 10720886]\n",
            " [     255  1491338]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByIpNhUk-xQr",
        "colab_type": "text"
      },
      "source": [
        "### Generate probability maps for the dataset using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zed1ERU_FyF",
        "colab_type": "code",
        "outputId": "5c8dcfc6-8407-4c63-d6da-5bea90d9bb87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage.measurements import label\n",
        "from PIL import Image\n",
        "from scipy.special import expit\n",
        "import os, sys\n",
        "\n",
        "saver = tf.train.import_meta_graph(\"drive/My Drive/data/models/model.ckpt.meta\")\n",
        "sess = tf.InteractiveSession()\n",
        "saver.restore(sess, \"drive/My Drive/data/models/model.ckpt\")\n",
        "X, mode = tf.get_collection(\"inputs\")[:2]\n",
        "pred = tf.get_collection(\"outputs\")[0]\n",
        "\n",
        "def read_image(image_path, gray=False):\n",
        "    \"\"\"Returns an image array\n",
        "    Args:\n",
        "        image_path (str): Path to image.jpg\n",
        "    Returns:\n",
        "        3-D array: RGB numpy image array\n",
        "    \"\"\"\n",
        "    if gray:\n",
        "        return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    image = cv2.imread(image_path)    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def pipeline(image, image_WH=(512, 512)):\n",
        "    image = np.copy(image)\n",
        "    H, W, C = image.shape\n",
        "    \n",
        "    if (W, H) != image_WH:\n",
        "        image = cv2.resize(image, image_WH)\n",
        "    \n",
        "    mask_pred = sess.run(pred, feed_dict={X: np.expand_dims(image, 0),\n",
        "                                          mode: False})\n",
        "    \n",
        "    mask_pred = np.squeeze(mask_pred)\n",
        "    mask_pred = expit(mask_pred)\n",
        "    # mask_pred = mask_pred > threshold\n",
        "    return mask_pred\n",
        "\n",
        "output_dir = \"drive/My Drive/data/output/prob/\"\n",
        "dir = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join(dir,output_dir)):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "for image_path in os.listdir(os.path.join(dir,\"drive/My Drive/data/output/images/\")):\n",
        "    image = read_image(\"drive/My Drive/data/output/images/\"+image_path)\n",
        "    predicted_image = np.zeros((2848, 4288), dtype=float)\n",
        "\n",
        "    for i in range(6):\n",
        "        for j in range(9):\n",
        "            top_y = i*512\n",
        "            if (i==5):\n",
        "                top_y = 2336\n",
        "            top_x = j*512\n",
        "            if (j==8):\n",
        "                top_x = 3776\n",
        "\n",
        "            image_crop = image[top_y:top_y+512, top_x:top_x+512]\n",
        "            predicted_crop = pipeline(image_crop)\n",
        "            predicted_image[top_y:top_y+512, top_x:top_x+512] = np.maximum(predicted_image[top_y:top_y+512, top_x:top_x+512], predicted_crop)\n",
        "\n",
        "    # threshold = 0.5\n",
        "    # predicted_image = predicted_image > threshold\n",
        "    #(unique, counts) = np.unique((predicted_image*255).astype('uint8'), return_counts=True)\n",
        "    #frequencies = np.asarray((unique, counts)).T\n",
        "    #print(frequencies)\n",
        "    predicted_save = Image.fromarray((predicted_image*255).astype('uint8'))\n",
        "    predicted_save.save(output_dir+image_path, \"JPEG\", quality=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/My Drive/data/models/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnNDIjEh_iD3",
        "colab_type": "text"
      },
      "source": [
        "### Generate segmented output masks from the probability maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37-KEnaJ_lxg",
        "colab_type": "code",
        "outputId": "9f6f6eac-84cc-47f8-d8b3-40806f487e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage.measurements import label\n",
        "from PIL import Image\n",
        "from scipy.special import expit\n",
        "import os, sys\n",
        "\n",
        "saver = tf.train.import_meta_graph(\"drive/My Drive/data/models/model.ckpt.meta\")\n",
        "sess = tf.InteractiveSession()\n",
        "saver.restore(sess, \"drive/My Drive/data/models/model.ckpt\")\n",
        "X, mode = tf.get_collection(\"inputs\")[:2]\n",
        "pred = tf.get_collection(\"outputs\")[0]\n",
        "\n",
        "def read_image(image_path, gray=False):\n",
        "    \"\"\"Returns an image array\n",
        "    Args:\n",
        "        image_path (str): Path to image.jpg\n",
        "    Returns:\n",
        "        3-D array: RGB numpy image array\n",
        "    \"\"\"\n",
        "    if gray:\n",
        "        return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    image = cv2.imread(image_path)    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def pipeline(image, image_WH=(512, 512)):\n",
        "    image = np.copy(image)\n",
        "    H, W, C = image.shape\n",
        "    \n",
        "    if (W, H) != image_WH:\n",
        "        image = cv2.resize(image, image_WH)\n",
        "    \n",
        "    mask_pred = sess.run(pred, feed_dict={X: np.expand_dims(image, 0),\n",
        "                                          mode: False})\n",
        "    \n",
        "    mask_pred = np.squeeze(mask_pred)\n",
        "    mask_pred = expit(mask_pred)\n",
        "    # mask_pred = mask_pred > threshold\n",
        "    return mask_pred\n",
        "\n",
        "output_dir = \"drive/My Drive/data/output/predicted/\"\n",
        "dir = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join(dir,output_dir)):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "for image_path in os.listdir(os.path.join(dir,\"drive/My Drive/data/output/prob/\")):\n",
        "    im_prob = Image.open(\"drive/My Drive/data/output/prob/\"+image_path)\n",
        "    arr_prob = (np.array(im_prob)).astype(float)/255\n",
        "    threshold = 0.7\n",
        "    arr_pred = (arr_prob > threshold).astype('uint8')\n",
        "    # image = read_image(\"test_data/\"+image_path)\n",
        "    # predicted_image = np.zeros((2848, 4288), dtype=float)\n",
        "\n",
        "    # for i in range(10):\n",
        "    #     for j in range(16):\n",
        "    #         top_y = i*256\n",
        "    #         if (i==9):\n",
        "    #             top_y = 2336\n",
        "    #         top_x = j*256\n",
        "    #         if (j==15):\n",
        "    #             top_x = 3776\n",
        "\n",
        "    #         image_crop = image[top_y:top_y+512, top_x:top_x+512]\n",
        "    #         predicted_crop = pipeline(image_crop)\n",
        "    #         predicted_image[top_y:top_y+512, top_x:top_x+512] = np.maximum(predicted_image[top_y:top_y+512, top_x:top_x+512], predicted_crop)\n",
        "    #(unique, counts) = np.unique(arr_pred*255, return_counts=True)\n",
        "    #frequencies = np.asarray((unique, counts)).T\n",
        "    #print(frequencies)\n",
        "    predicted_save = Image.fromarray(arr_pred*255)\n",
        "    predicted_save.save(output_dir+image_path, \"JPEG\", quality=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/My Drive/data/models/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjewf0kzHheN",
        "colab_type": "text"
      },
      "source": [
        "### Calculate sensitivity and precison values for individual images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfPnyrHXHm4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, sys\n",
        "\n",
        "training_size=80\n",
        "gt_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "pred_dir = \"drive/My Drive/data/output/predicted/\"\n",
        "sn = np.empty(training_size, dtype=float)\n",
        "ppv = np.empty(training_size, dtype=float)\n",
        "sp = np.empty(training_size, dtype=float)\n",
        "image_paths = np.empty(training_size, dtype=object)\n",
        "\n",
        "dir = os.getcwd()\n",
        "i=0\n",
        "for image_path in os.listdir(os.path.join(dir,gt_dir)):\n",
        "\timage_paths[i] = image_path\n",
        "\tim_gt = Image.open(gt_dir+image_path)\n",
        "\timg = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "\tim_pred = Image.open(pred_dir+img)\n",
        "\tarr_gt = np.array(im_gt)\n",
        "\tarr_pred = np.array(im_pred)\n",
        "\tarr_pred = arr_pred > 0\n",
        "\t#(unique, counts) = np.unique(arr_gt, return_counts=True)\n",
        "\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t#print(\"arr_gt: \",frequencies)\n",
        "\t#(unique, counts) = np.unique(arr_pred, return_counts=True)\n",
        "\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t#print(\"arr_pred: \",frequencies)\n",
        "\ttrue_p = np.sum(np.logical_and(arr_gt, arr_pred))\n",
        "\tactual_p = np.sum(arr_gt)\n",
        "\tpred_p = np.sum(arr_pred)\n",
        "\t\n",
        "\tfalse_p = pred_p - true_p\n",
        "\tactual_n = 4288*2848 - actual_p\n",
        "\ttrue_n = actual_n - false_p\n",
        "\t#print (\"True pos: \", true_p)\n",
        "\t#print (\"Actual pos: \", actual_p)\n",
        "\t#print (\"Pred pos: \", pred_p)\n",
        "\tif actual_p == 0:\n",
        "\t\tsn[i] = 1\n",
        "\telse:\n",
        "\t\tsn[i] = float(true_p)/float(actual_p)\n",
        "\tif pred_p == 0:\n",
        "\t\tppv[i] = 1\n",
        "\telse:\n",
        "\t\tppv[i] = float(true_p)/float(pred_p)\n",
        "\t#print (i)\n",
        "\tif actual_n == 0:\n",
        "\t\tsp[i] = 1\n",
        "\telse:\n",
        "\t\tsp[i] = float(true_n)/float(actual_n)\n",
        "\ti+=1\n",
        "\n",
        "sn_csv = np.stack((image_paths,sn), axis=1)\n",
        "ppv_csv = np.stack((image_paths,ppv), axis=1)\n",
        "sp_csv = np.stack((image_paths,sp), axis=1)\n",
        "\n",
        "np.savetxt(\"drive/My Drive/data/sn.csv\", sn_csv, delimiter=\",\", fmt=\"%s\")\n",
        "np.savetxt(\"drive/My Drive/data/ppv.csv\", ppv_csv, delimiter=\",\", fmt=\"%s\")\n",
        "np.savetxt(\"drive/My Drive/data/sp.csv\", sp_csv, delimiter=\",\", fmt=\"%s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM2NY0sRHysI",
        "colab_type": "text"
      },
      "source": [
        "### plot FROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq8hygaTHxv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_size=80\n",
        "gt_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "prob_dir = \"drive/My Drive/data/output/prob/\"\n",
        "true_p=0\n",
        "actual_p=0\n",
        "pred_p=0\n",
        "false_p=0\n",
        "\n",
        "thresh_list = [0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999, 0.9999, 0.99999, 1]\n",
        "\n",
        "dir = os.getcwd()\n",
        "thresh_size = len(thresh_list)\n",
        "sn = np.empty(thresh_size, dtype=float)\n",
        "fppi = np.empty(thresh_size, dtype=float)\n",
        "thresh_array = np.array(thresh_list)\n",
        "\n",
        "for th in range(thresh_size):\n",
        "\tthreshold = thresh_array[th]\n",
        "\tprint (threshold)\n",
        "\ttrue_p=0\n",
        "\tactual_p=0\n",
        "\tpred_p=0\n",
        "\tfalse_p=0\n",
        "\n",
        "\tfor image_path in os.listdir(os.path.join(dir,gt_dir)):\n",
        "\t\t# print image_path\n",
        "\t\tim_gt = Image.open(gt_dir+image_path)\n",
        "\t\timg = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "\t\tim_prob = Image.open(prob_dir+img)\n",
        "\t\tarr_gt = np.array(im_gt)\n",
        "\t\t#(unique, counts) = np.unique(arr_gt, return_counts=True)\n",
        "\t\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t\t#print(\"arr_gt: \",frequencies)\n",
        "\t\tarr_prob = (np.array(im_prob)).astype(float)/255\n",
        "\t\t#(unique, counts) = np.unique(arr_prob, return_counts=True)\n",
        "\t\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t\t#print(\"arr_prob: \",frequencies)\n",
        "\t\tarr_pred = (arr_prob > threshold).astype('uint8')\n",
        "\t\t#(unique, counts) = np.unique(arr_pred, return_counts=True)\n",
        "\t\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t\t#print(\"arr_pred: \",frequencies)\n",
        "\t\ttp = np.sum(np.logical_and(arr_gt, arr_pred))\n",
        "\t\tap = np.sum(arr_gt)\n",
        "\t\tpp = np.sum(arr_pred)\n",
        "\t\ttrue_p += tp\n",
        "\t\tactual_p += ap\n",
        "\t\tpred_p += pp\n",
        "\t\tfalse_p += (pp-tp)\n",
        "\n",
        "\tsn[th] = float(true_p)/float(actual_p)\n",
        "\tprint (\"sn: \", sn[th])\n",
        "\tfppi[th] = float(false_p)/float(training_size)\n",
        "\tprint (\"fppi: \", fppi[th])\n",
        "\n",
        "plt.plot(fppi, sn)\n",
        "plt.ylabel('SN')\n",
        "plt.xlabel('FPs per image')\n",
        "plt.savefig('drive/My Drive/data/froc.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hwL8cLfH89I",
        "colab_type": "text"
      },
      "source": [
        "### compute average statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJaPdfF89P7x",
        "colab_type": "code",
        "outputId": "0288c6ba-2055-412d-cc53-d9ea0e0c02d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "#np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "gt_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "pred_dir = \"drive/My Drive/data/output/predicted/\"\n",
        "true_p=0\n",
        "actual_p=0\n",
        "pred_p=0\n",
        "false_p=0\n",
        "false_n=0\n",
        "actual_n=0\n",
        "true_n=0\n",
        "pred_n=0\n",
        "\n",
        "dir = os.getcwd()\n",
        "\n",
        "for image_path in os.listdir(os.path.join(dir,gt_dir)):\n",
        "  im_gt = Image.open(gt_dir+image_path)\n",
        "  img = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "  im_pred = Image.open(pred_dir+img)\n",
        "  arr_gt = np.array(im_gt)\n",
        "  arr_pred = np.array(im_pred)\n",
        "  arr_pred = arr_pred > 0\n",
        "  #(unique, counts) = np.unique(arr_pred, return_counts=True)\n",
        "  #frequencies = np.asarray((unique, counts)).T\n",
        "  #print(\"arr_pred: \",frequencies)\n",
        "\n",
        "  tp = np.sum(np.logical_and(arr_gt, arr_pred))\n",
        "  #print(\"tp: \",tp)\n",
        "  ap = np.sum(arr_gt)\n",
        "  #print(\"ap: \",ap)\n",
        "  pp = np.sum(arr_pred)\n",
        "\n",
        "  fp = pp - tp\n",
        "  an = 4288*2848 - ap\n",
        "  pn = 4288*2848 - pp\n",
        "  tn = an - fp\n",
        "  fn = pn - tn\n",
        "\n",
        "  true_p += tp\n",
        "  actual_p += ap\n",
        "  pred_p += pp\n",
        "  pred_n += pn\n",
        "  false_p += fp\n",
        "  actual_n += an\n",
        "  true_n += tn\n",
        "  false_n +=fn\n",
        "\n",
        "\n",
        "sn = float(true_p)/float(actual_p)\n",
        "ppv = float(true_p)/float(pred_p)\n",
        "sp = float(true_n)/float(actual_n)\n",
        "npv = float(true_n)/float(pred_n)\n",
        "acc = float(true_p + true_n)/float(actual_p + actual_n)\n",
        "f1 = float(2*true_p)/float((2*true_p)+false_p+false_n)\n",
        "\n",
        "print (\"Sensitivity/Recall/True Positive Rate(TPR)(TP/P): \", sn)\n",
        "print (\"Precision/Positive Predictive Value(PPV)(TP/TP+FP): \", ppv)\n",
        "print (\"Specificity/Selectivity/True Negative Rate(TNR)(TN/N): \", sp)\n",
        "print (\"Negative Predictive Value(NPV)(TN/TN+FN): \", npv)\n",
        "print (\"Accuracy(TP+TN/P+N)\",acc)\n",
        "print (\"F1 Score(2TP/2TP+FP+FN)\",f1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitivity/Recall/True Positive Rate(TPR)(TP/P):  0.3003172405641447\n",
            "Precision/Positive Predictive Value(PPV)(TP/TP+FP):  0.7103850647835652\n",
            "Specificity/Selectivity/True Negative Rate(TNR)(TN/N):  0.9987316581469773\n",
            "Negative Predictive Value(NPV)(TN/TN+FN):  0.9927948866965305\n",
            "Accuracy(TP+TN/P+N) 0.9915707798186473\n",
            "F1 Score(2TP/2TP+FP+FN) 0.4221636405991698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9LjYv56IQ7Q",
        "colab_type": "text"
      },
      "source": [
        "### precision score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ott8N-YVISa9",
        "colab_type": "code",
        "outputId": "35379d67-3638-4002-b145-7861a784cd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, sys\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "training_size=80\n",
        "gt_dir = \"drive/My Drive/data/output/annotations/\"\n",
        "prob_dir = \"drive/My Drive/data/output/prob/\"\n",
        "\n",
        "dir = os.getcwd()\n",
        "\n",
        "i=0\n",
        "sum_pav=0\n",
        "for image_path in os.listdir(os.path.join(dir,gt_dir)):\n",
        "\t# print image_path\n",
        "\tim_gt = Image.open(gt_dir+image_path)\n",
        "\timg = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "\tim_prob = Image.open(prob_dir+img)\n",
        "\tarr_gt = (np.array(im_gt)).astype(bool)\n",
        "\tarr_prob = (np.array(im_prob)).astype(float)/255\n",
        "\t#(unique, counts) = np.unique(arr_gt, return_counts=True)\n",
        "\t#frequencies = np.asarray((unique, counts)).T\n",
        "\t#print(\"arr_prob: \",frequencies)\n",
        "\tpav = average_precision_score(arr_gt.reshape((-1)),arr_prob.reshape((-1)))\n",
        "\tsum_pav = sum_pav+pav\n",
        "\ti = i+1\n",
        "\n",
        "mpav = sum_pav/i\n",
        "print(mpav)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3873802591232976\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}